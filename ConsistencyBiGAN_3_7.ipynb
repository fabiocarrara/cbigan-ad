{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Wt8qIXvo2L1"
   },
   "source": [
    "# First of all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FoNMw11OYl6u"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fFMMgxJ8vDVt"
   },
   "outputs": [],
   "source": [
    "!pip install keras==2.2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7sZDnVHx08Ew"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ngGL4TxLnAP4"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYYaLnSb2JrY"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from math import floor\n",
    "import numpy as np\n",
    "import time\n",
    "from functools import partial\n",
    "from random import random\n",
    "import os\n",
    "from keras.layers import Conv2D, Dense, AveragePooling2D, Activation, Cropping2D, Dropout, BatchNormalization\n",
    "from keras.layers import Reshape, UpSampling2D, Flatten, Input, add, Lambda, concatenate, LeakyReLU, multiply\n",
    "from keras.preprocessing.image import load_img, img_to_array, array_to_img, ImageDataGenerator\n",
    "from keras.layers import GlobalAveragePooling2D, average\n",
    "from keras.models import model_from_json, Model\n",
    "from keras.initializers import VarianceScaling\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from typing import Tuple\n",
    "from enum import Enum\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import copy\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iye-usQdoxQ6"
   },
   "source": [
    "# Bidirectional GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "1gb9r380m1Ds"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from math import floor\n",
    "import numpy as np\n",
    "import time\n",
    "from functools import partial\n",
    "from random import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from keras.models import Sequential, Model\n",
    "\n",
    "#from keras.layers import Input, Reshape, Dense, Dropout, MaxPooling2D, Conv2D, Flatten\n",
    "#from keras.layers import Conv2DTranspose, LeakyReLU\n",
    "\n",
    "#from keras.layers.core import Activation\n",
    "\n",
    "from keras.layers import Conv2D, Dense, AveragePooling2D, Activation, Cropping2D, Dropout, BatchNormalization\n",
    "from keras.layers import Reshape, UpSampling2D, Flatten, Input, add, Lambda, concatenate, LeakyReLU, multiply\n",
    "from keras.layers import GlobalAveragePooling2D, average\n",
    "from keras.models import model_from_json, Model\n",
    "from keras.initializers import VarianceScaling\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "from keras.preprocessing.image import load_img, img_to_array, array_to_img, ImageDataGenerator\n",
    "\n",
    "from sklearn.feature_extraction import image\n",
    "from sklearn.datasets import load_sample_image\n",
    "import cv2\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "###GLOBAL VARIABLE###\n",
    "im_size = 64\n",
    "latent_size = 64\n",
    "BATCH_SIZE = 32\n",
    "directory = \"Object\"\n",
    "suff = 'png'\n",
    "cmode = 'RGB'\n",
    "channels = 3\n",
    "k_images = 3\n",
    "cha = 16\n",
    "object_name = \"Leather_2\"\n",
    "alpha = 0.0000001\n",
    "alphac = 1-alpha\n",
    "\n",
    "##FLAGS##\n",
    "size_adjusted = False\n",
    "DATA_AUGMENTATION = False\n",
    "PATCHES = True\n",
    "LOAD_MODEL = False\n",
    "#####################\n",
    "\n",
    "def noise(n):\n",
    "    return np.random.normal(0.0, 1.0, size = [n, latent_size])\n",
    "\n",
    "class dataGenerator(object):\n",
    "\n",
    "    def __init__(self, loc, flip = False, suffix = 'png'):\n",
    "        self.flip = False\n",
    "        self.suffix = suffix\n",
    "        self.files = []\n",
    "        self.n = 1e10\n",
    "\n",
    "        print(\"Importing Images...\")\n",
    "\n",
    "        try:\n",
    "            os.mkdir(\"/content/drive/My Drive/Colab Notebooks/BiGAN/Images/\"+str(object_name))\n",
    "            os.mkdir(\"/content/drive/My Drive/Colab Notebooks/BiGAN/\"+str(object_name))\n",
    "        except:\n",
    "            pass\n",
    "            #self.load_from_npy(loc)\n",
    "            #return\n",
    "\n",
    "        DATASET_TRAIN_PATH = \"/content/drive/My Drive/Colab Notebooks/test_AD_1/leather/train/good\"\n",
    "        for dirpath, dirnames, filenames in os.walk(DATASET_TRAIN_PATH):\n",
    "            for filename in [f for f in filenames if f.endswith(\".\"+str('png'))]:\n",
    "                print('\\r' + str(len(self.files)), end = '\\r')\n",
    "\n",
    "                # CREATE THE PATCH OF THE IMAGE\n",
    "                fname = os.path.join(dirpath, filename)\n",
    "                #print(fname)\n",
    "\n",
    "                if not PATCHES:\n",
    "                  #######WITH OUT PATCHES#######\n",
    "                  # LOAD THE IMAGE WHEN WE NOT USE THE PATCHES \n",
    "                  temp = Image.open(fname).convert(cmode)\n",
    "                  temp = temp.resize((im_size, im_size), Image.BILINEAR)\n",
    "                  temp = np.array(temp, dtype='uint8')\n",
    "\n",
    "                  self.files.append(temp)\n",
    "                  ##############################\n",
    "                else:\n",
    "                  ##########WITH PATCHES########\n",
    "                  # USE THIS INSTEAD WHEN WE MUS DIVIDE THE IMAGES IN PATCHES \n",
    "                  dim_img_resize = 512\n",
    "                  img = load_img(fname, target_size=(dim_img_resize, dim_img_resize))\n",
    "                  img = np.array(img)\n",
    "\n",
    "                  #patches = image.extract_patches_2d(img, (256, 256), max_patches=4)\n",
    "                  #print(patches.shape)\n",
    "\n",
    "                  # DIVIDE THE IMAGE IN PATCHES OF DIMENSION 128X128\n",
    "                  patches_dimension = 64\n",
    "                  blocks = np.array([img[i:i+patches_dimension, j:j+patches_dimension] for j in range(0,dim_img_resize,patches_dimension) for i in range(0,dim_img_resize,patches_dimension)])\n",
    "                  \n",
    "                  # PUT EACH PATCHES EXTRACTED FROM THE IMAGES IN AN ARRAY \n",
    "                  for elem in blocks:\n",
    "                    self.files.append(elem)\n",
    "                  ##############################\n",
    "\n",
    "        self.files = np.array(self.files)\n",
    "\n",
    "        # PLOT A SAMPLE OF THE TRAINING SET\n",
    "        plt.imshow(self.files[0])\n",
    "        plt.show()\n",
    "\n",
    "        if DATA_AUGMENTATION:\n",
    "          # DATA AUGMENTATION\n",
    "          # CREATE VARIETY IN THE TRAINING SET IN ORDER TO TRAIN THE GAN \n",
    "          datagen = ImageDataGenerator(\n",
    "            rotation_range=0,\n",
    "            zoom_range=0,\n",
    "            width_shift_range=0,\n",
    "            height_shift_range=0,\n",
    "            #shear_range=0.15,\n",
    "            horizontal_flip=False,\n",
    "            fill_mode=\"nearest\")\n",
    "          \n",
    "          imageGen = datagen.flow(self.files, batch_size=1)\n",
    "          img = imageGen.next()\n",
    "          new_dataset = []\n",
    "          for i in range(0,50000):\n",
    "            image = np.squeeze(img, axis = 0)\n",
    "            #plt.imshow(image.astype('uint8'))\n",
    "            #plt.show()\n",
    "            new_dataset.append(image)\n",
    "            img = imageGen.next()\n",
    "\n",
    "          self.files = np.array(new_dataset)\n",
    "          \n",
    "        self.n = self.files.shape[0]\n",
    "\n",
    "        print(\"Found \" + str(self.n) + \" images in \" + loc + \".\")\n",
    "\n",
    "    def load_from_npy(self, loc):\n",
    "\n",
    "        print(\"Loading from .npy files.\")\n",
    "\n",
    "        self.files = np.load(\"data/\" + str(loc) + \"-npy-\" + str(im_size) + \"/data.npy\")\n",
    "\n",
    "        self.n = self.files.shape[0]\n",
    "\n",
    "\n",
    "    def get_batch(self, num):\n",
    "\n",
    "        idx = np.random.randint(0, self.n - 200, num)\n",
    "        out = []\n",
    "\n",
    "        for i in range(num):\n",
    "            out.append(self.files[idx[i]])\n",
    "\n",
    "        return np.array(out).astype('float32') / 255.0\n",
    "\n",
    "    def get_test_batch(self, num):\n",
    "\n",
    "        idx = np.random.randint(self.n - 200, self.n, num)\n",
    "        out = []\n",
    "\n",
    "        for i in range(num):\n",
    "            out.append(self.files[idx[i]])\n",
    "\n",
    "        return np.array(out).astype('float32') / 255.0\n",
    "\n",
    "\n",
    "# Print iterations progress\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 50, fill = '█'):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print('\\r %s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = '\\r')\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total:\n",
    "        print()\n",
    "        print()\n",
    "\n",
    "def gradient_penalty_loss(y_true, y_pred, averaged_samples, weight):\n",
    "    gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "    gradients_sqr = K.square(gradients)\n",
    "    gradient_penalty = K.sum(gradients_sqr,\n",
    "                              axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "\n",
    "    # (weight / 2) * ||grad||^2\n",
    "    # Penalize the gradient norm\n",
    "    return K.mean(gradient_penalty) * (weight / 2)\n",
    "\n",
    "def hinge_d(y_true, y_pred):\n",
    "    return K.mean(K.relu(1.0 - (y_true * y_pred)))\n",
    "\n",
    "def w_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)\n",
    "\n",
    "def sum_of_residual(y_true, y_pred):\n",
    "    return K.sum(K.abs(y_true - y_pred))\n",
    "\n",
    "def g_block(inp, fil, u = True):\n",
    "    if u:\n",
    "        out = UpSampling2D(interpolation = 'bilinear')(inp)\n",
    "    else:\n",
    "        out = Activation('linear')(inp)\n",
    "\n",
    "    skip = Conv2D(fil, 1, padding = 'same', kernel_initializer = 'he_normal')(out)\n",
    "\n",
    "    out = Conv2D(filters = fil, kernel_size = 3, padding = 'same', kernel_initializer = 'he_normal')(out)\n",
    "    out = LeakyReLU(0.2)(out)\n",
    "\n",
    "    out = Conv2D(filters = fil, kernel_size = 3, padding = 'same', kernel_initializer = 'he_normal')(out)\n",
    "    out = LeakyReLU(0.2)(out)\n",
    "\n",
    "    out = Conv2D(fil, 1, padding = 'same', kernel_initializer = 'he_normal')(out)\n",
    "\n",
    "    out = add([out, skip])\n",
    "    out = LeakyReLU(0.2)(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "def d_block(inp, fil, p = True):\n",
    "    skip = Conv2D(fil, 1, padding = 'same', kernel_initializer = 'he_normal')(inp)\n",
    "\n",
    "    out = Conv2D(filters = fil, kernel_size = 3, padding = 'same', kernel_initializer = 'he_normal')(inp)\n",
    "    out = LeakyReLU(0.2)(out)\n",
    "\n",
    "    out = Conv2D(filters = fil, kernel_size = 3, padding = 'same', kernel_initializer = 'he_normal')(out)\n",
    "    out = LeakyReLU(0.2)(out)\n",
    "\n",
    "    out = Conv2D(fil, 1, padding = 'same', kernel_initializer = 'he_normal')(out)\n",
    "\n",
    "    out = add([out, skip])\n",
    "    out = LeakyReLU(0.2)(out)\n",
    "\n",
    "    if p:\n",
    "        out = AveragePooling2D()(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "class GAN(object):\n",
    "\n",
    "    def __init__(self, steps = 1, lr = 0.0001, decay = 0.00001):\n",
    "\n",
    "        #Models\n",
    "        self.D = None\n",
    "        self.E = None\n",
    "        self.G = None\n",
    "\n",
    "        self.GE = None\n",
    "        self.EE = None\n",
    "\n",
    "        self.DM = None\n",
    "        self.AM = None\n",
    "\n",
    "        #Config\n",
    "        self.LR = lr\n",
    "        self.steps = steps\n",
    "        self.beta = 0.999\n",
    "\n",
    "        #Init Models\n",
    "        self.discriminator()\n",
    "        self.generator()\n",
    "        self.encoder()\n",
    "\n",
    "        self.EE = model_from_json(self.E.to_json())\n",
    "        self.EE.set_weights(self.E.get_weights())\n",
    "\n",
    "        self.GE = model_from_json(self.G.to_json())\n",
    "        self.GE.set_weights(self.G.get_weights())\n",
    "\n",
    "    def discriminator(self):\n",
    "\n",
    "        if self.D:\n",
    "            return self.D\n",
    "\n",
    "        inp = Input(shape = [im_size, im_size, 3])\n",
    "        inpl = Input(shape = [latent_size])\n",
    "\n",
    "        #Latent input\n",
    "        l = Dense(512, kernel_initializer = 'he_normal')(inpl)\n",
    "        l = LeakyReLU(0.2)(l)\n",
    "        l = Dense(512, kernel_initializer = 'he_normal')(l)\n",
    "        l = LeakyReLU(0.2)(l)\n",
    "        l = Dense(512, kernel_initializer = 'he_normal')(l)\n",
    "        l = LeakyReLU(0.2)(l)\n",
    "\n",
    "        #Image input\n",
    "        x = d_block(inp, 1 * cha)   #64\n",
    "        x = d_block(x, 2 * cha)   #32\n",
    "        x = d_block(x, 3 * cha)   #16\n",
    "        x = d_block(x, 4 * cha)  #8\n",
    "        x = d_block(x, 8 * cha)  #4\n",
    "        x = d_block(x, 16 * cha, p = False)  #4\n",
    "\n",
    "        x = Flatten()(x)\n",
    "\n",
    "        x = concatenate([x, l])\n",
    "\n",
    "        x = Dense(16 * cha, kernel_initializer = 'he_normal')(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "\n",
    "        x = Dense(1, kernel_initializer = 'he_normal')(x)\n",
    "\n",
    "        self.D = Model(inputs = [inp, inpl], outputs = x)\n",
    "        print(\"Discriminator\")\n",
    "        self.D.summary()\n",
    "\n",
    "        return self.D\n",
    "\n",
    "    def generator(self):\n",
    "\n",
    "        if self.G:\n",
    "            return self.G\n",
    "\n",
    "        #Inputs\n",
    "        inp = Input(shape = [latent_size])\n",
    "\n",
    "        #Latent\n",
    "\n",
    "        #Actual Model\n",
    "        x = Dense(2*2*16*cha, kernel_initializer = 'he_normal')(inp)\n",
    "        x = Reshape([2, 2, 16*cha])(x)\n",
    "\n",
    "        x = g_block(x, 16 * cha, u = False)  #4\n",
    "        x = g_block(x, 8 * cha)  #8\n",
    "        x = g_block(x, 4 * cha)  #16\n",
    "        x = g_block(x, 3 * cha)   #32\n",
    "        x = g_block(x, 2 * cha)   #64\n",
    "        x = g_block(x, 1 * cha)   #128\n",
    "\n",
    "        x = Conv2D(filters = 3, kernel_size = 1, activation = 'sigmoid', padding = 'same', kernel_initializer = 'he_normal')(x)\n",
    "\n",
    "        self.G = Model(inputs = inp, outputs = x)\n",
    "        print(\"Generator\")\n",
    "        self.G.summary()\n",
    "\n",
    "        return self.G\n",
    "\n",
    "    def encoder(self):\n",
    "\n",
    "        if self.E:\n",
    "            return self.E\n",
    "\n",
    "        inp = Input(shape = [im_size, im_size, 3])\n",
    "\n",
    "        x = d_block(inp, 1 * cha)   #64\n",
    "        x = d_block(x, 2 * cha)   #32\n",
    "        x = d_block(x, 3 * cha)   #16\n",
    "        x = d_block(x, 4 * cha)  #8\n",
    "        x = d_block(x, 8 * cha)  #4\n",
    "        x = d_block(x, 16 * cha, p = False)  #4\n",
    "\n",
    "        x = Flatten()(x)\n",
    "\n",
    "        x = Dense(16 * cha, kernel_initializer = 'he_normal')(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "\n",
    "        x = Dense(latent_size, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(x)\n",
    "\n",
    "        self.E = Model(inputs = inp, outputs = x)\n",
    "        print(\"Encoder\")\n",
    "        self.E.summary()\n",
    "\n",
    "        return self.E\n",
    "\n",
    "    def AdModel(self):\n",
    "\n",
    "        #D does not update\n",
    "        self.D.trainable = False\n",
    "        for layer in self.D.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        #G does update\n",
    "        self.G.trainable = True\n",
    "        for layer in self.G.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "        #E does update\n",
    "        self.E.trainable = True\n",
    "        for layer in self.E.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "        # E(x), x\n",
    "        ri = Input(shape = [im_size, im_size, 3])\n",
    "        er = self.E(ri)\n",
    "        dr = self.D([ri, er])\n",
    "\n",
    "        # z, G(z)\n",
    "        gi = Input(shape = [latent_size])\n",
    "        gf = self.G(gi)\n",
    "        df = self.D([gf, gi])\n",
    "\n",
    "        # LOSS CONSISTENCY IMAGE\n",
    "        # L1_NOMR( X - G(E(X)) )\n",
    "        ci = self.G(self.E(ri))\n",
    "\n",
    "        # LOSS CONSISTENCY LATENT\n",
    "        # L1_NORM( Z - E(G(Z)) )\n",
    "        cl = self.E(self.G(gi))\n",
    "\n",
    "        # INPUT OF THE MODEL:\n",
    "        #   RI: REAL IMAGE\n",
    "        #   GI: REAL LATENT \n",
    "        # OUTPUT OF THE MODEL:\n",
    "        #   DF: D(z, G(z))\n",
    "        #   GR: D(E(x), x) \n",
    "        #   C: [G(E(X)),E(G(Z))]\n",
    "        self.AM = Model(\n",
    "            inputs = [ri, gi], \n",
    "            outputs = [dr, df, ci, cl]\n",
    "            )\n",
    "\n",
    "        # ORIGINA SETTINGS:\n",
    "        # B1 = 0\n",
    "        # B2 = 0.099\n",
    "        self.AM.compile(\n",
    "            optimizer = Adam(self.LR, beta_1 = 0, beta_2 = 0.099), \n",
    "            loss = [w_loss, w_loss, sum_of_residual, sum_of_residual], \n",
    "            loss_weights=[alphac, alphac, alpha, alpha]\n",
    "            )\n",
    "\n",
    "        return self.AM\n",
    "\n",
    "    def DisModel(self):\n",
    "\n",
    "        #D does update\n",
    "        self.D.trainable = True\n",
    "        for layer in self.D.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "        #G does not update\n",
    "        self.G.trainable = False\n",
    "        for layer in self.G.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        #E does update\n",
    "        self.E.trainable = False\n",
    "        for layer in self.E.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        # Fake Latent / Real Image\n",
    "        # x, REAL IMAGE\n",
    "        ri = Input(shape = [im_size, im_size, 3])\n",
    "        # E(x)\n",
    "        er = self.E(ri)\n",
    "        # E(x), x\n",
    "        dr = self.D([ri, er])\n",
    "\n",
    "        # Real Latent / Fake Image\n",
    "        # z, REAL LATENT SPACE\n",
    "        gi = Input(shape = [latent_size])\n",
    "        # G(z)\n",
    "        gf = self.G(gi)\n",
    "        # G(z), z\n",
    "        df = self.D([gf, gi])\n",
    "\n",
    "        self.DM = Model(\n",
    "            inputs = [ri, gi], \n",
    "            outputs = [dr, df, df]\n",
    "            )\n",
    "\n",
    "        # Create partial of gradient penalty loss\n",
    "        # For r1, averaged_samples = ri\n",
    "        # For r2, averaged_samples = gf\n",
    "        # Weight of 10 typically works\n",
    "        partial_gp_loss = partial(gradient_penalty_loss, averaged_samples = [gf, gi], weight = 5)\n",
    "\n",
    "        #Compile With Corresponding Loss Functions\n",
    "        self.DM.compile(\n",
    "            optimizer = Adam(self.LR, beta_1 = 0, beta_2 = 0.909), \n",
    "            loss=[hinge_d, hinge_d, partial_gp_loss]\n",
    "            )\n",
    "\n",
    "        return self.DM\n",
    "\n",
    "    def EMA(self):\n",
    "\n",
    "        start = time.clock()\n",
    "\n",
    "        for i in range(len(self.G.layers)):\n",
    "            up_weight = self.G.layers[i].get_weights()\n",
    "            old_weight = self.GE.layers[i].get_weights()\n",
    "            new_weight = []\n",
    "            for j in range(len(up_weight)):\n",
    "                new_weight.append(old_weight[j] * self.beta + (1-self.beta) * up_weight[j])\n",
    "            self.GE.layers[i].set_weights(new_weight)\n",
    "\n",
    "        for i in range(len(self.E.layers)):\n",
    "            up_weight = self.E.layers[i].get_weights()\n",
    "            old_weight = self.EE.layers[i].get_weights()\n",
    "            new_weight = []\n",
    "            for j in range(len(up_weight)):\n",
    "                new_weight.append(old_weight[j] * self.beta + (1-self.beta) * up_weight[j])\n",
    "            self.EE.layers[i].set_weights(new_weight)\n",
    "\n",
    "        #print(\"Moved Average. \" + str(time.clock() - start) + \"s\")\n",
    "\n",
    "    def MAinit(self):\n",
    "        self.EE.set_weights(self.E.get_weights())\n",
    "        self.GE.set_weights(self.G.get_weights())\n",
    "\n",
    "class BiGAN(object):\n",
    "\n",
    "    def __init__(self, steps = 1, lr = 0.0001, decay = 0.00001, silent = True):\n",
    "\n",
    "        self.GAN = GAN(steps = steps, lr = lr, decay = decay)\n",
    "        self.DisModel = self.GAN.DisModel()\n",
    "        self.AdModel = self.GAN.AdModel()\n",
    "\n",
    "        # LOAD THE LAST MODEL SAVED TO CONTINUE THE TRAINING FROM \"step\" ITERATION \n",
    "        if LOAD_MODEL:\n",
    "          try:\n",
    "            num = self.get_last_epoch(object_name)\n",
    "            num = 1\n",
    "            self.load(num)\n",
    "            print(\"IMPORTED PRE-TRAINED MODEL AT EPOCH \" + str(num))\n",
    "          except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "        self.lastblip = time.clock()\n",
    "\n",
    "        self.noise_level = 0\n",
    "\n",
    "        self.im = dataGenerator(directory, suffix = suff, flip = True)\n",
    "\n",
    "        self.silent = silent\n",
    "\n",
    "        #Train Generator to be in the middle, not all the way at real. Apparently works better??\n",
    "        self.ones = np.ones((BATCH_SIZE, 1), dtype=np.float32)\n",
    "        self.zeros = np.zeros((BATCH_SIZE, 1), dtype=np.float32)\n",
    "        self.nones = -self.ones\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        #Train Alternating\n",
    "        a = self.train_dis()\n",
    "        b = self.train_gen()\n",
    "\n",
    "        if self.GAN.steps % 10 == 0:\n",
    "            self.GAN.EMA()\n",
    "\n",
    "        if self.GAN.steps == 10000:\n",
    "            self.GAN.MAinit()\n",
    "\n",
    "        #Print info\n",
    "        if self.GAN.steps % 100 == 0 and not self.silent:\n",
    "            print(\"\\n\\nRound \" + str(self.GAN.steps) + \":\")\n",
    "            print(\"D: \" + str(a))\n",
    "            print(\"G: \" + str(b))\n",
    "            s = round((time.clock() - self.lastblip), 4)\n",
    "            steps_per_second = 100 / s\n",
    "            steps_per_minute = steps_per_second * 60\n",
    "            steps_per_hour = steps_per_minute * 60\n",
    "            print(\"Steps/Second: \" + str(round(steps_per_second, 2)))\n",
    "            print(\"Steps/Hour: \" + str(round(steps_per_hour)))\n",
    "            min1k = floor(1000/steps_per_minute)\n",
    "            sec1k = floor(1000/steps_per_second) % 60\n",
    "            print(\"1k Steps: \" + str(min1k) + \":\" + str(sec1k))\n",
    "            self.lastblip = time.clock()\n",
    "            steps_left = 200000 - self.GAN.steps + 1e-7\n",
    "            hours_left = steps_left // steps_per_hour\n",
    "            minutes_left = (steps_left // steps_per_minute) % 60\n",
    "\n",
    "            print(\"Til Completion: \" + str(int(hours_left)) + \"h\" + str(int(minutes_left)) + \"m\")\n",
    "            print()\n",
    "\n",
    "            #Save Model\n",
    "            if self.GAN.steps % 1000 == 0:\n",
    "                self.save(floor(self.GAN.steps / 1000))\n",
    "\n",
    "            if self.GAN.steps % 1000 == 0 or (self.GAN.steps % 100 == 0 and self.GAN.steps < 1000):\n",
    "                self.evaluate(floor(self.GAN.steps / 1000))\n",
    "\n",
    "\n",
    "        printProgressBar(self.GAN.steps % 100, 99, decimals = 0)\n",
    "\n",
    "        self.GAN.steps = self.GAN.steps + 1\n",
    "\n",
    "    def train_dis(self):\n",
    "\n",
    "        #Get Data\n",
    "        train_data = [self.im.get_batch(BATCH_SIZE), noise(BATCH_SIZE)]\n",
    "\n",
    "        #Train\n",
    "        d_loss = self.DisModel.train_on_batch(train_data, [self.ones, self.nones, self.ones])\n",
    "\n",
    "        return d_loss\n",
    "\n",
    "    def train_gen(self):\n",
    "\n",
    "        #Train\n",
    "        train_data = [self.im.get_batch(BATCH_SIZE), noise(BATCH_SIZE)]\n",
    "\n",
    "        g_loss = self.AdModel.train_on_batch(train_data, [self.ones, self.nones, train_data[0], train_data[1]])\n",
    "\n",
    "        return g_loss\n",
    "\n",
    "    def evaluate(self, num = 0):\n",
    "\n",
    "        num = 1\n",
    "\n",
    "        n1 = noise(32)\n",
    "\n",
    "        generated_images = self.GAN.G.predict(n1, batch_size = BATCH_SIZE)\n",
    "\n",
    "        real_images = self.im.get_test_batch(16)\n",
    "        latent_codes = self.GAN.E.predict(real_images, batch_size = BATCH_SIZE)\n",
    "        reconstructed_images = self.GAN.G.predict(latent_codes, batch_size = BATCH_SIZE)\n",
    "\n",
    "        print(\"E Mean: \" + str(np.mean(latent_codes)))\n",
    "        print(\"E Std: \" + str(np.std(latent_codes)))\n",
    "        print(\"E Std Featurewise: \" + str(np.mean(np.std(latent_codes, axis = 0))))\n",
    "        print()\n",
    "\n",
    "        r = []\n",
    "\n",
    "        # BATCH OF GENERATED IMAGES FROM GENERATOR WITH OUT ENCODER \n",
    "        for i in range(0, 32, 8):\n",
    "            r.append(np.concatenate(generated_images[i:i+8], axis = 1))\n",
    "\n",
    "        hline = np.zeros([16, 8 * im_size, 3])\n",
    "        r.append(hline)\n",
    "\n",
    "        for i in range(0, 16, 8):\n",
    "            r.append(np.concatenate(real_images[i:i+8], axis = 1))\n",
    "            r.append(np.concatenate(reconstructed_images[i:i+8], axis = 1))\n",
    "\n",
    "        c1 = np.concatenate(r, axis = 0)\n",
    "\n",
    "        x = Image.fromarray(np.uint8(c1*255))\n",
    "\n",
    "        x.save(\"/content/drive/My Drive/Colab Notebooks/BiGAN/Images/\"+object_name+\"/i\"+str(num)+\".png\")\n",
    "\n",
    "        # Moving Average\n",
    "        if False:\n",
    "          n1 = noise(32)\n",
    "\n",
    "          generated_images = self.GAN.GE.predict(n1, batch_size = BATCH_SIZE)\n",
    "\n",
    "          latent_codes = self.GAN.EE.predict(real_images, batch_size = BATCH_SIZE)\n",
    "          reconstructed_images = self.GAN.GE.predict(latent_codes, batch_size = BATCH_SIZE)\n",
    "\n",
    "          r = []\n",
    "\n",
    "          for i in range(0, 32, 8):\n",
    "              r.append(np.concatenate(generated_images[i:i+8], axis = 1))\n",
    "\n",
    "          hline = np.zeros([16, 8 * im_size, 3])\n",
    "          r.append(hline)\n",
    "\n",
    "          for i in range(0, 16, 8):\n",
    "              r.append(np.concatenate(real_images[i:i+8], axis = 1))\n",
    "              r.append(np.concatenate(reconstructed_images[i:i+8], axis = 1))\n",
    "\n",
    "          c1 = np.concatenate(r, axis = 0)\n",
    "\n",
    "          x = Image.fromarray(np.uint8(c1*255))\n",
    "\n",
    "          x.save(\"/content/drive/My Drive/Colab Notebooks/BiGAN/Images/\"+object_name+\"/i\"+str(num)+\"-ema.png\")\n",
    "\n",
    "\n",
    "    def prepareSamples(self, cnum = 0, num = 1000): #8x8 images, bottom row is constant\n",
    "\n",
    "        try:\n",
    "            os.mkdir(\"Results/Samples-c\" + str(cnum))\n",
    "        except:\n",
    "            x = 0\n",
    "\n",
    "        im = self.im.get_class(cnum)\n",
    "        e = self.GAN.E.predict(im, batch_size = BATCH_SIZE * k_images)\n",
    "\n",
    "        mean = np.mean(e, axis = 0)\n",
    "        std = np.std(e, axis = 0)\n",
    "\n",
    "        n = noise(num)\n",
    "        nc = nClass(num, mean, std)\n",
    "\n",
    "        im = self.GAN.G.predict([n, nc], batch_size = BATCH_SIZE)\n",
    "\n",
    "        for i in range(im.shape[0]):\n",
    "\n",
    "            x = Image.fromarray(np.uint8(im[i]*255), mode = 'RGB')\n",
    "\n",
    "            x.save(\"Results/Samples-c\" + str(cnum) + \"/im (\"+str(i+1)+\").png\")\n",
    "\n",
    "    def saveModel(self, model, name, num):\n",
    "        num = 1\n",
    "        json = model.to_json()\n",
    "        with open(\"/content/drive/My Drive/Colab Notebooks/BiGAN/\"+object_name+\"/\"+name+\".json\", \"w\") as json_file:\n",
    "            json_file.write(json)\n",
    "\n",
    "        model.save_weights(\"/content/drive/My Drive/Colab Notebooks/BiGAN/\"+object_name+\"/\"+name+\"_\"+str(num)+\".h5\")\n",
    "        #model.save(\"Models/\"+name+\"_\"+str(num)+\".h5\")\n",
    "\n",
    "    def loadModel(self, name, num):\n",
    "\n",
    "        file = open(\"/content/drive/My Drive/Colab Notebooks/BiGAN/\"+object_name+\"/\"+name+\".json\", 'r')\n",
    "        json = file.read()\n",
    "        file.close()\n",
    "\n",
    "        mod = model_from_json(json)\n",
    "        mod.load_weights(\"/content/drive/My Drive/Colab Notebooks/BiGAN/\"+object_name+\"/\"+name+\"_\"+str(num)+\".h5\")\n",
    "\n",
    "        return mod\n",
    "\n",
    "    def save(self, num): #Save JSON and Weights into /Models/\n",
    "        num = 1\n",
    "        self.saveModel(self.GAN.G, \"gen\", num)\n",
    "        self.saveModel(self.GAN.D, \"dis\", num)\n",
    "        self.saveModel(self.GAN.E, \"enc\", num)\n",
    "\n",
    "        self.saveModel(self.GAN.GE, \"genMA\", num)\n",
    "        self.saveModel(self.GAN.EE, \"encMA\", num)\n",
    "\n",
    "\n",
    "    def load(self, num): #Load JSON and Weights from /Models/\n",
    "        steps1 = self.GAN.steps\n",
    "\n",
    "        #Load Models\n",
    "        self.GAN.G = self.loadModel(\"gen\", num)\n",
    "        self.GAN.D = self.loadModel(\"dis\", num)\n",
    "        self.GAN.E = self.loadModel(\"enc\", num)\n",
    "\n",
    "        self.GAN.GE = self.loadModel(\"genMA\", num)\n",
    "        self.GAN.EE = self.loadModel(\"encMA\", num)\n",
    "\n",
    "        self.GAN.steps = steps1\n",
    "\n",
    "        self.DisModel = self.GAN.DisModel()\n",
    "        self.AdModel = self.GAN.AdModel()\n",
    "\n",
    "    # FUNCTION TO GET THE LAST NUMBER OF EPOCH IN ORDET TO GET\n",
    "    # TRAINED MODEL \n",
    "    def get_last_epoch(self, o):\n",
    "      path = \"/content/drive/My Drive/Colab Notebooks/BiGAN/Images/\"\n",
    "      path = path + o\n",
    "\n",
    "      arr = []\n",
    "      for elem in os.listdir(path):\n",
    "        if \"ema\" not in elem and elem != \".ipynb_checkpoints\":\n",
    "          num = int((elem.split(\".\")[0]).replace(\"i\", \"\"))\n",
    "          arr.append(num)\n",
    "\n",
    "      return np.max(np.array(arr))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #np.random.seed(1)\n",
    "    model = BiGAN(lr = 0.0001, silent = False)\n",
    "    #model.evaluate(0)\n",
    "\n",
    "    while model.GAN.steps <= 600000:\n",
    "      model.train()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EfSgU10nhv5"
   },
   "source": [
    "# Anomaly detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66gKyC_B0N4y"
   },
   "outputs": [],
   "source": [
    "# CREATE A RANDOM NORMAL NOISE THAT IS THE INPUT OF THE GENERATOR\n",
    "def mynoise(n):\n",
    "    return np.random.normal(0.0, 1.0, size = [n, 64])\n",
    "\n",
    "# CLASS TO DEFINE SOME PARAMETERS \n",
    "class DataParm(Enum):\n",
    "    common_path = './model'\n",
    "    common_path_efficient = './model2'\n",
    "    image_size_127_5 = 127.5\n",
    "    batch_size_64 = 64\n",
    "\n",
    "# LOSS FUNCTION OF THE ANOMALY DETECTOR, THIS IS USED TO FIND THE \n",
    "# POINT z' IN THE LATENT SPACE Z THAT GENERATE THE MOST SIMILAR \n",
    "# IMAGE TO THE QUERY \n",
    "def sum_of_residual(y_true: np.array, y_pred: np.array):\n",
    "    return K.sum(K.abs(y_true - y_pred))\n",
    "\n",
    "class FeatureExtractorParam(Enum):\n",
    "    loss_binary_crossentropy = \"binary_crossentropy\"\n",
    "    optimizer_rmsprop = \"rmsprop\"\n",
    "    feature_extract_layer_first_conv = -65\n",
    "\n",
    "# FEATURE EXTRACTOR, THIS IS USED TO CREATE THE FEATURE \n",
    "# REPRESENTATION OF THE QUERY AND FOR THE GENERATED IMAGE\n",
    "def feature_extractor(d: Model=None, common_path: str=DataParm.common_path.value) -> Model:\n",
    "    intermidiate_model = Model(inputs=d.layers[0].input, outputs=d.layers[FeatureExtractorParam.feature_extract_layer_first_conv.value].output)\n",
    "    intermidiate_model.compile(loss=FeatureExtractorParam.loss_binary_crossentropy.value, optimizer=FeatureExtractorParam.optimizer_rmsprop.value)\n",
    "\n",
    "    return intermidiate_model\n",
    "\n",
    "class AnomalyDetectorParam(Enum):\n",
    "    activation_sigmoid = \"sigmoid\"\n",
    "    optimizer_rmsprop = 'rmsprop'\n",
    "\n",
    "# ANOMALY DETECTOR\n",
    "def anomaly_detector(g: Model, d: Model, e: Model, in_shape: tuple=(64,), loss_weights: list=[0.9, 0.1]) -> Model:\n",
    "    intermidiate_model = feature_extractor(d)\n",
    "    feature_extractor.trainable = False\n",
    "\n",
    "    g = Model(inputs=g.layers[1].input, outputs=g.layers[-1].output)\n",
    "    g.trainable = False\n",
    "    \n",
    "    a2Input = Input(shape=in_shape)\n",
    "\n",
    "    gInput = Dense(in_shape[0], trainable=True)(a2Input)\n",
    "    gInput = Activation(AnomalyDetectorParam.activation_sigmoid.value)(gInput)\n",
    "    \n",
    "    G_out = g(gInput)\n",
    "    D_out = intermidiate_model(G_out)\n",
    "    \n",
    "    model = Model(inputs=a2Input, outputs=[G_out, D_out])\n",
    "    model.compile(loss=sum_of_residual, loss_weights=loss_weights, optimizer=AnomalyDetectorParam.optimizer_rmsprop.value)\n",
    "    \n",
    "    K.set_learning_phase(0)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# COMPUTE THE ANOMALY SCORE OF THE QUERY USING THE ANOMALY DETECTOR,\n",
    "# WE ITERARE OVER Z IN ORDER TO FIND THE MOST SIMILAR IMAGE TO THE QUERY,\n",
    "# IN THE LAST ITERATION WE CAN COMPUTE THE LOSS \n",
    "def compute_anomaly_score(model: Model, x: np.array, iterations: int=500, d: Model=None, e: Model=None, noize_shape: tuple=(1, 64)) -> Tuple[np.asarray, np.asarray,]:\n",
    "    z = np.random.uniform(0, 1, size=noize_shape)\n",
    "    intermidiate_model = feature_extractor(d)\n",
    "    d_x = intermidiate_model.predict(x)\n",
    "    # X E D_X SONO IL TARGET DELLA NEURAL NETWORK, \n",
    "    # ESSA ANDRA' A EFFETTUARE LA PREDICTION, ANDANDO A GENERARE\n",
    "    # G(Z) E FARE POI IL MAPPING CON D\n",
    "    # LA LOSS SARA' DATA DALLA SOMMA PESATA TRA \n",
    "    # k(x-x') + (1-k)(d_x - d_x')\n",
    "    loss = model.fit(z, [x, d_x], batch_size=1, epochs=iterations, verbose=0)\n",
    "    similar_data, _ = model.predict(z)\n",
    "    loss = loss.history['loss'][-1]\n",
    "    \n",
    "    return loss, similar_data\n",
    "\n",
    "# GIVEN THE QUERY WE COMPUTE THE ANOMALY SCORE, WE FIND THE MOST SIMILAR IMAGE\n",
    "# AND WE PERFORM THE DIFFERENCE BETWEEN THE QUERY AND THE SIMILAR IMAGE \n",
    "def anomaly_detection(test_img: np.asarray, g: Model, d: Model, e: Model, iteration: int=200) -> Tuple[np.asarray, np.asarray, np.asarray, np.asarray,]:\n",
    "    model = anomaly_detector(g=g, d=d, e=e)\n",
    "    test_img_shape = test_img.shape\n",
    "    \n",
    "    ano_score, similar_img = compute_anomaly_score(model, test_img.reshape(1, test_img_shape[0], test_img_shape[1], test_img_shape[2]), iterations=iteration, d=d, e=e)\n",
    "    \n",
    "    np_residual = test_img.reshape(test_img_shape[0], test_img_shape[1], test_img_shape[2]) - similar_img.reshape(test_img_shape[0], test_img_shape[1], test_img_shape[2])\n",
    "    np_residual = (np_residual + 2) / 4\n",
    "    \n",
    "    np_residual = (255 * np_residual).astype(np.uint8)\n",
    "    #original_x = (test_img.reshape(test_img_shape[0], test_img_shape[1], test_img_shape[2]) * DataParm.image_size_127_5.value + DataParm.image_size_127_5.value).astype(np.uint8)\n",
    "    #similiar_x = (similar_img.reshape(test_img_shape[0], test_img_shape[1], test_img_shape[2]) * DataParm.image_size_127_5.value + DataParm.image_size_127_5.value).astype(np.uint8)\n",
    "    \n",
    "    #original_x_color = cv2.cvtColor(original_x, cv2.COLOR_GRAY2BGR)\n",
    "    #residual_color = cv2.applyColorMap(np_residual, cv2.COLORMAP_JET)\n",
    "    #show = cv2.addWeighted(original_x_color, 0.3, residual_color, 0.7, 0.)\n",
    "\n",
    "    sim = cv2.cvtColor(similar_img[0],cv2.COLOR_BGR2GRAY)\n",
    "    test = cv2.cvtColor(test_img,cv2.COLOR_BGR2GRAY)\n",
    "    errors = np.abs(sim-test)\n",
    "    #errors = sim-test\n",
    "    #diff = np.mean(np.squeeze(errors),axis=2)\n",
    "    \n",
    "    return ano_score, test_img, similar_img[0], errors\n",
    "\n",
    "# BELOW WE HAVE A VARIATION OF THE CALCULATION OF THE ANOMALY SCORE,\n",
    "# NOW IS COMPUTED AS A MEAN OF MORE ANOMALY SCORE \n",
    "def anomaly_detector_second(g: Model, d: Model, in_shape: tuple=(64,), loss_weights: list=[0.9, 0.1]) -> Model:\n",
    "\n",
    "    intermidiate_model = feature_extractor(d)\n",
    "    feature_extractor.trainable = False\n",
    "\n",
    "    g = Model(inputs=g.layers[1].input, outputs=g.layers[-1].output)\n",
    "    g.trainable = False\n",
    "\n",
    "    aInput = Input(shape=in_shape)\n",
    "\n",
    "    gInput = Dense(in_shape[0], trainable=True)(aInput)\n",
    "    gInput = Activation(AnomalyDetectorParam.activation_sigmoid.value)(gInput)\n",
    "    \n",
    "    G_out = g(gInput)\n",
    "    D_out = intermidiate_model(G_out)\n",
    "\n",
    "    model = Model(inputs=aInput, outputs=[G_out, D_out])\n",
    "    model.compile(loss=sum_of_residual, loss_weights=loss_weights, optimizer=AnomalyDetectorParam.optimizer_rmsprop.value)\n",
    "    \n",
    "    K.set_learning_phase(0)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def compute_anomaly_score_second(model: Model, x: np.array, iterations: int=5, z_num: int=5, d: Model=None, g: Model=None, noize_shape: tuple=(1, 64)) -> Tuple[np.asarray, np.asarray,]:\n",
    "    loss_list = []\n",
    "    similar_data_list = []\n",
    "    initial_g = copy.deepcopy(g)\n",
    "    \n",
    "    for i in range(z_num):\n",
    "        z = np.random.uniform(0, 1, size=noize_shape)\n",
    "        intermidiate_model = feature_extractor(d)\n",
    "        d_x = intermidiate_model.predict(x)\n",
    "        loss = model.fit(z, [x, d_x], batch_size=1, epochs=iterations, verbose=0)\n",
    "        similar_data, _ = model.predict(z)\n",
    "        loss = loss.history['loss'][-1]\n",
    "        loss_list.append(loss)\n",
    "        similar_data_list.append(similar_data)\n",
    "        model = anomaly_detector_second(g=initial_g, d=d)\n",
    "      \n",
    "    average_loss = np.average(loss_list)\n",
    "    average_similar_data = np.average(similar_data_list, axis=0)\n",
    "    \n",
    "    return average_loss, average_similar_data\n",
    "\n",
    "def anomaly_detection_second(test_img, g=None, d=None, iterations: int=5) -> Tuple[np.asarray, np.asarray, np.asarray, np.asarray,]:\n",
    "    model = anomaly_detector_second(g=g, d=d)\n",
    "    test_img_shape = test_img.shape\n",
    "\n",
    "    ano_score, similar_img = compute_anomaly_score_second(model, test_img.reshape(1, test_img_shape[0], test_img_shape[1], test_img_shape[2]), iterations=iterations, d=d, g=g)\n",
    "    \n",
    "    np_residual = test_img.reshape(test_img_shape[0], test_img_shape[1], test_img_shape[2]) - similar_img.reshape(test_img_shape[0], test_img_shape[1], test_img_shape[2])\n",
    "    np_residual = (np_residual + 2) / 4\n",
    "    \n",
    "    np_residual = (255 * np_residual).astype(np.uint8)\n",
    "    sim = cv2.cvtColor(similar_img[0],cv2.COLOR_BGR2GRAY)\n",
    "    test = cv2.cvtColor(test_img,cv2.COLOR_BGR2GRAY)\n",
    "    errors = np.abs(sim-test)\n",
    "    \n",
    "    return ano_score, test_img, similar_img[0], errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxMuS6XBbCRr"
   },
   "source": [
    "# Anomaly detector using **Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ep8xKuSDBDqN"
   },
   "outputs": [],
   "source": [
    "def mynoise(n):\n",
    "    return np.random.normal(0.0, 1.0, size = [n, 64])\n",
    "\n",
    "def sum_of_residual(y_true: np.array, y_pred: np.array):\n",
    "    return K.sum(K.abs(y_true - y_pred))\n",
    "\n",
    "class DataParm(Enum):\n",
    "    common_path = './model'\n",
    "    common_path_efficient = './model2'\n",
    "    image_size_127_5 = 127.5\n",
    "    batch_size_64 = 64\n",
    "\n",
    "class FeatureExtractorEncodeParam(Enum):\n",
    "    feature_extractor_loss = 'binary_crossentropy'\n",
    "    optimizer = 'adam'\n",
    "    \n",
    "class AnomalyDetectorEncodeParam(Enum):\n",
    "    adam = 'adam'\n",
    "\n",
    "def feature_extractor_encode(d: Model=None, common_path: str=\"Models\") -> Model:\n",
    "    input_layer = []\n",
    "\n",
    "    for index, each_d in enumerate(d.layers):\n",
    "        if 'input' in each_d.name:\n",
    "            input_layer.append(each_d.input)\n",
    "    \n",
    "    intermidiate_model = Model(inputs=input_layer, outputs=d.layers[-3].output)\n",
    "    intermidiate_model.compile(loss=FeatureExtractorEncodeParam.feature_extractor_loss.value, optimizer=FeatureExtractorEncodeParam.optimizer.value)\n",
    "\n",
    "    #plot_model(intermidiate_model, to_file=\"anomaly_detector_encoder.png\")\n",
    "    \n",
    "    return intermidiate_model\n",
    "\n",
    "def anomaly_detector_encode(g: Model, d: Model, e: Model, img_shape: tuple=(256, 256, 3), loss_weights: list=[0.9, 0.1]) -> Model:\n",
    "    intermidiate_model = feature_extractor_encode(d)\n",
    "    intermidiate_model.trainable = False\n",
    "    #g = Model(inputs=g.layers[1].input, outputs=g.layers[-1].output)\n",
    "\n",
    "    g.trainable = False\n",
    "    e.trainable = False\n",
    "    \n",
    "    #e = Model(inputs=e.layers[1].input, outputs=e.layers[-1].output)\n",
    "    aInput = Input(shape=img_shape)\n",
    "\n",
    "    E_out = e(aInput)\n",
    "    G_out = g(E_out)\n",
    "    D_out = intermidiate_model([G_out, E_out])\n",
    "\n",
    "    model = Model(inputs=aInput, outputs=[G_out, D_out])\n",
    "    model.compile(loss=sum_of_residual, loss_weights=loss_weights, optimizer=AnomalyDetectorEncodeParam.adam.value)\n",
    "    \n",
    "    K.set_learning_phase(0)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def l2_norm(x, y, axis=None):\n",
    "    if axis is None:\n",
    "        return tf.reduce_sum(tf.pow(x-y, 2))\n",
    "    else:\n",
    "        return tf.reduce_sum(tf.pow(x-y, 2), axis=axis)\n",
    "\n",
    "def MSE(x, y, axis=None):\n",
    "    if axis is None:\n",
    "        return tf.reduce_mean(tf.pow(x-y, 2))\n",
    "    else:\n",
    "        return tf.reduce_mean(tf.pow(x-y, 2), axis=axis)\n",
    "\n",
    "def compute_anomaly_score_encode(model: Model, intermidiate_model: Model, x: np.array, noise: np.array, d: Model=None, e: Model=None, g: Model=None) -> Tuple[np.asarray, np.asarray,]:\n",
    "    # LATENT REPRESENTATION OF THE TEST SAMPLE: E(x)\n",
    "    generate_noise = e.predict(x)\n",
    "\n",
    "    # GENERATE IMAGE WITH LATENT REPRESENTATION OF SAMPLE TEST: G(E(x))\n",
    "    generate_image = g.predict(generate_noise)\n",
    "    \n",
    "    # FEATURE REPRESENTATION OF GENERATED IMAGE AND TEST SAMPLE\n",
    "    # f(x, E(x))\n",
    "    target_d_x = intermidiate_model.predict([x, generate_noise])\n",
    "    # f(G(E(x)), z)\n",
    "    d_x = intermidiate_model.predict([generate_image, generate_noise])\n",
    "\n",
    "    if False:\n",
    "      loss = model.evaluate(generate_image, [x, target_d_x], batch_size=1, verbose=0)\n",
    "      loss = loss[-1]\n",
    "      similar_data, _ = model.predict(generate_image)\n",
    "    else:\n",
    "      # COMPUTE THE DIFFERENCE PIXEL PER PIXEL BETWEEN\n",
    "      # THE QUERY AND THE GENERATED IMAGE\n",
    "      loss_image = [np.abs(each_x - g_x) for each_x, g_x in zip(x, generate_image)]\n",
    "      sum_loss_image = np.sum(loss_image)\n",
    "\n",
    "      # COMPUTE THE DIFFERENCE THE FEATURE REPRESENTATION \n",
    "      # OF THE QUERY AND OF THE GENERATED IMAGE\n",
    "      loss_feature = [np.abs(each_d_x - each_g_d_x) for each_d_x, each_g_d_x in zip(target_d_x, d_x)]\n",
    "      sum_loss_feature = np.sum(loss_feature)\n",
    "\n",
    "      # COMPUTE THE LOSS AS THE WEIGHTED SUM OF THE \n",
    "      # RESIDUAL LOSS AND THE DISCRIMINATOR LOSS\n",
    "      loss = 0.9*sum_loss_image + 0.1*sum_loss_feature\n",
    "    \n",
    "    return loss, generate_image\n",
    "\n",
    "def anomaly_detection_encode(test_img, noise, g=None, d=None, e=None, model=None, intermidiate_model=None) -> Tuple[np.asarray, np.asarray, np.asarray, np.asarray,]:\n",
    "    test_img_shape = test_img.shape\n",
    "\n",
    "    ano_score, similar_img = compute_anomaly_score_encode(model, intermidiate_model, test_img.reshape(1, test_img_shape[0], test_img_shape[1], test_img_shape[2]), noise, d=d, e=e, g=g)\n",
    "\n",
    "    np_residual = test_img.reshape(test_img_shape[0], test_img_shape[1], test_img_shape[2]) - similar_img.reshape(test_img_shape[0], test_img_shape[1], test_img_shape[2])\n",
    "    np_residual = (np_residual + 2) / 4\n",
    "    \n",
    "    np_residual = (255 * np_residual).astype(np.uint8)\n",
    "\n",
    "    sim = cv2.cvtColor(similar_img[0],cv2.COLOR_BGR2GRAY)\n",
    "    test = cv2.cvtColor(test_img,cv2.COLOR_BGR2GRAY)\n",
    "    errors = np.abs(sim-test)\n",
    "    \n",
    "    return ano_score, test_img, similar_img[0], errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uF59TH7mbJDy"
   },
   "source": [
    "# Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fw-AxqOa3RUn"
   },
   "outputs": [],
   "source": [
    "def loadModel(name, num, object_name):\n",
    "    #CABLE: 12\n",
    "    #Toothbrush: 4\n",
    "    #Screw: 100\n",
    "\n",
    "    file = open(\"/content/drive/My Drive/Colab Notebooks/BiGAN/\"+object_name+\"/\"+name+\".json\", 'r')\n",
    "    json = file.read()\n",
    "    file.close()\n",
    "\n",
    "    mod = model_from_json(json)\n",
    "    mod.load_weights(\"/content/drive/My Drive/Colab Notebooks/BiGAN/\"+object_name+\"/\"+name+\"_\"+str(num)+\".h5\")\n",
    "\n",
    "    return mod\n",
    "\n",
    "def get_test_data(DATASET_TRAIN_PATH, im_size):\n",
    "  #DATASET_TRAIN_PATH = \"/content/drive/My Drive/Colab Notebooks/test_AD_1/\"+object_name+\"/test/broken_small\"\n",
    "  X_test = []\n",
    "  cmode = \"RGB\"\n",
    "  size_adjusted = False\n",
    "\n",
    "  for dirpath, dirnames, filenames in os.walk(DATASET_TRAIN_PATH):\n",
    "      for filename in [f for f in filenames if f.endswith(\".\"+str('png'))]:\n",
    "          print('\\r' + str(len(X_test)), end = '\\r')\n",
    "          \n",
    "          fname = os.path.join(dirpath, filename)\n",
    "\n",
    "          temp = Image.open(fname).convert(cmode)\n",
    "          if not size_adjusted:\n",
    "              temp = temp.resize((im_size, im_size), Image.BILINEAR)\n",
    "\n",
    "          temp = np.array(temp, dtype='uint8')\n",
    "          X_test.append(temp)\n",
    "\n",
    "  X_test = np.array(X_test)\n",
    "  X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "  return X_test\n",
    "\n",
    "# FUNCTION TO GET THE LAST NUMBER OF EPOCH IN ORDET TO GET\n",
    "# TRAINED MODEL \n",
    "def get_last_epoch(o):\n",
    "  path = \"/content/drive/My Drive/Colab Notebooks/BiGAN/Images/\"\n",
    "  path = path + o\n",
    "\n",
    "  arr = []\n",
    "  for elem in os.listdir(path):\n",
    "    if \"ema\" not in elem and elem != \".ipynb_checkpoints\":\n",
    "      num = int((elem.split(\".\")[0]).replace(\"i\", \"\"))\n",
    "      arr.append(num)\n",
    "\n",
    "  return np.max(np.array(arr))\n",
    "\n",
    "# CREATE A DICTONARY THAT CONTAIN FOR EACH OBJECT THE TEST SAMPLE\n",
    "# WE HAVE 2 DIFFERENT DICTONARY TO MANTAIN THE IMAGE IN 128X128 AND 256X256\n",
    "def createDataset(dim):\n",
    "  dataset = {}\n",
    "  for object_name in os.listdir(\"/content/drive/My Drive/Colab Notebooks/test_AD_1/\"):\n",
    "    print(\"\\n\\n#  ACTUAL OBJECT: {} #\".format(object_name))\n",
    "    dataset[object_name] = {}\n",
    "    for dirnames in os.listdir(\"/content/drive/My Drive/Colab Notebooks/test_AD_1/\"+object_name+\"/\"):\n",
    "      try:\n",
    "        if(dirnames != \"ground_truth\" and dirnames != \"license.txt\" and dirnames != \"readme.txt\" and dirnames != \"train\"):\n",
    "          print(\"\\n\\t###  ACTUAL DIRECTORY: {} ###\".format(dirnames))\n",
    "          dataset[object_name][dirnames] = {}\n",
    "          for sub_dirnames in os.listdir(os.path.join(\"/content/drive/My Drive/Colab Notebooks/test_AD_1/\"+object_name+\"/\", dirnames)):\n",
    "            dir_path = os.path.join(\"/content/drive/My Drive/Colab Notebooks/test_AD_1/\"+object_name+\"/\", dirnames, sub_dirnames)\n",
    "            print(\"\\t\\t\" + dir_path)\n",
    "            X_test_1024 = get_test_data(dir_path, dim)\n",
    "            dataset[object_name][dirnames][sub_dirnames] =  X_test_1024\n",
    "      except Exception as e:\n",
    "        pass\n",
    "\n",
    "  return dataset\n",
    "\n",
    "def data_augmentation(data):\n",
    "  # DATA AUGMENTATION\n",
    "  # CREATE VARIETY IN THE TRAINING SET IN ORDER TO TRAIN THE GAN \n",
    "  datagen = ImageDataGenerator(\n",
    "    rotation_range=0,\n",
    "    zoom_range=0,\n",
    "    width_shift_range=0,\n",
    "    height_shift_range=0,\n",
    "    #shear_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\")\n",
    "  \n",
    "  imageGen = datagen.flow(data, batch_size=1)\n",
    "  img = imageGen.next()\n",
    "  new_dataset = []\n",
    "\n",
    "  for i in range(0,500):\n",
    "    image = np.squeeze(img, axis = 0)\n",
    "    new_dataset.append(image)\n",
    "    img = imageGen.next()\n",
    "\n",
    "  new_dataset = np.array(new_dataset)\n",
    "\n",
    "  return new_dataset\n",
    "\n",
    "def TSNE_visualization(normal_objects, outlier_objects, object_name):\n",
    "  encoder = False\n",
    "  TSNE_FLAG = False\n",
    "\n",
    "  num = get_last_epoch(object_name)\n",
    "  e = loadModel(\"enc\", num, object_name)\n",
    "  d = loadModel(\"dis\", num, object_name)\n",
    "  if encoder:\n",
    "    model = feature_extractor_encode(d)\n",
    "  else:\n",
    "    model = feature_extractor(d)\n",
    "\n",
    "  feature_map_good = []\n",
    "  feature_map_anomaly = []\n",
    "\n",
    "  #normal_objects = data_augmentation(normal_objects)\n",
    "  #outlier_objects = data_augmentation(outlier_objects)\n",
    "\n",
    "  # EXTRACT FEATURE FOR NORMAIL OBJECTS\n",
    "  for query in normal_objects:  \n",
    "    query_img_shape = query.shape\n",
    "    query = query.reshape(1, query_img_shape[0], query_img_shape[1], query_img_shape[2])\n",
    "\n",
    "    if encoder:\n",
    "      generate_noise = e.predict(query)\n",
    "      feature_map_good.append(model.predict([query, generate_noise]))\n",
    "    else:\n",
    "      feature_map_good.append(model.predict(query))\n",
    "\n",
    "  # EXTRACT FEATURE FOR OUTLIER OBJECTS\n",
    "  for query in outlier_objects:  \n",
    "    query_img_shape = query.shape\n",
    "    query = query.reshape(1, query_img_shape[0], query_img_shape[1], query_img_shape[2])\n",
    "\n",
    "    if encoder:\n",
    "      generate_noise = e.predict(query)\n",
    "      feature_map_anomaly.append(model.predict([query, generate_noise]))\n",
    "    else:\n",
    "      feature_map_anomaly.append(model.predict(query))\n",
    "\n",
    "  # CRETE PLOT TO VISUALIZZE NORMAL OBJECTS AND OUTLIER OBJECT \n",
    "  if len(feature_map_good) > 0 and len(feature_map_anomaly) > 0:\n",
    "    np_feature_map_good = np.array(feature_map_good)\n",
    "    np_feature_map_anomaly = np.array(feature_map_anomaly)\n",
    "\n",
    "    output = np.concatenate((np_feature_map_good, np_feature_map_anomaly))\n",
    "    output = output.reshape(output.shape[0], -1)\n",
    "    if TSNE_FLAG:\n",
    "      X_embedded = TSNE(n_components = 2).fit_transform(output)\n",
    "    else:\n",
    "      X_embedded = PCA(n_components = 2).fit_transform(output)\n",
    "\n",
    "    plt.figure(7)\n",
    "    plt.title(object_name.upper())\n",
    "    plt.scatter(X_embedded[:len(feature_map_good), 0], X_embedded[:len(feature_map_good), 1], label='normal')\n",
    "    plt.scatter(X_embedded[len(feature_map_good):, 0], X_embedded[len(feature_map_good):, 1], label='anomaly')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "#TSNE_visualization(dataset_256[\"transistor\"][\"test\"][\"good\"], dataset_256[\"transistor\"][\"test\"][\"misplaced\"], \"Transistor\")\n",
    "#dataset_512 = createDataset(512)\n",
    "\n",
    "# LOAD THE DATASET\n",
    "#object_name = \"Bottle\"\n",
    "#X_test = get_test_data(lower(object_name))\n",
    "\n",
    "# LOAD THE PRETRAINED MODEL\n",
    "#num = 22\n",
    "#g = loadModel(\"gen\", num, object_name)\n",
    "#d = loadModel(\"dis\", num, object_name)\n",
    "#e = loadModel(\"enc\", num, object_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PKS19AHxqeWQ"
   },
   "outputs": [],
   "source": [
    "# Save\n",
    "#np.save('/content/drive/My Drive/Colab Notebooks/mvtec_dataset/my_dataset_512.npy', dataset_512) \n",
    "#np.save('/content/drive/My Drive/Colab Notebooks/mvtec_dataset/my_dataset_224.npy', dataset_224) \n",
    "\n",
    "# Load\n",
    "dataset_256 = np.load('/content/drive/My Drive/Colab Notebooks/mvtec_dataset/my_dataset_256.npy', allow_pickle='TRUE').item()\n",
    "dataset_128 = np.load('/content/drive/My Drive/Colab Notebooks/mvtec_dataset/my_dataset_128.npy', allow_pickle='TRUE').item()\n",
    "#dataset_1024 = np.load('/content/drive/My Drive/Colab Notebooks/mvtec_dataset/my_dataset_1024.npy', allow_pickle='TRUE').item()\n",
    "dataset_512 = np.load('/content/drive/My Drive/Colab Notebooks/mvtec_dataset/my_dataset_512.npy', allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCzn68WjPCbH"
   },
   "source": [
    "# Evaluation of the anomaly detector\n",
    "\n",
    "---\n",
    "\n",
    "In this section we test the anomaly detector with the object that we have used to train the BiGAN. We have two different anomaly detector, the first that use the encoder to perform the detection in order to reduce the execution time (in this case we have a more efficient version but less accurate) and the second that is a normal anomaly detector that use a latent space optimization to find a good candiate image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDIrdkbMUam6"
   },
   "source": [
    "# Testing anomaly detector with patches\n",
    "---\n",
    "Evaluation of the texture, each images is divided in patches of nxn pixel. For each patch is performed the detection of anomalies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upmlChwDUZQb"
   },
   "outputs": [],
   "source": [
    "# FLAG FOR USE THE ANOMALY DETECTOR WITH OT WITH OUT THE ENCODER\n",
    "encoder = True\n",
    "consistency = False\n",
    "\n",
    "# ZOOMED IMAGE DIMENSION \n",
    "dim_img_resize = 512\n",
    "# PATCHES DIMENSION\n",
    "patches_dimension = 128\n",
    "dataset = dataset_512\n",
    "\n",
    "# LIST OF ALL OBJECT THAT WE MUST EVALUATE \n",
    "object_list = [\"carpet\"]\n",
    "\n",
    "# DICTONARY TO SAVE ALL THE SCORE OBTAINED FOR EACH OBJECT\n",
    "partial_list_of_scores = {}\n",
    "partial_list_of_y = {}\n",
    "\n",
    "# CREATE THE FOLDERS TO SAVE THE RESULT \n",
    "try:\n",
    "  os.mkdir(\"./result/\")\n",
    "  os.mkdir(\"./result/wEncoder\")\n",
    "  os.mkdir(\"./result/woEncoder\")\n",
    "except:\n",
    "  pass\n",
    "\n",
    "for o in object_list:\n",
    "  # TEMPORARY ARRAY TO STORE THE SCORE COMPUTED FOR EACH OBJECT\n",
    "  score_list = []\n",
    "  Y_list = []\n",
    "\n",
    "  print(\"\\n\\nACTUAL OBJECT: \" + o)\n",
    "\n",
    "  try:\n",
    "    os.mkdir(\"./result/wEncoder/\"+str(o))\n",
    "    os.mkdir(\"./result/woEncoder/\"+str(o))\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  try:\n",
    "    os.mkdir(\"./result/wEncoder/\"+str(o)+\"/testing/\")\n",
    "    os.mkdir(\"./result/woEncoder/\"+str(o)+\"/testing/\")\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  partial_list_of_scores[o] = {}\n",
    "  partial_list_of_y[o] = {}\n",
    "\n",
    "  if consistency:\n",
    "    object_name = o.capitalize()\n",
    "    # LOAD THE TRAINED MODEL FOR THE ACRUAL OBJECT\n",
    "    num = get_last_epoch(object_name + \"_2\")\n",
    "    num = 2\n",
    "    print(\"LOADED MODEL: \" + str(num))\n",
    "    g = loadModel(\"gen\", num, object_name + \"_2\")\n",
    "    e = loadModel(\"enc\", num, object_name + \"_2\")\n",
    "    d = loadModel(\"dis\", num, object_name + \"_2\")\n",
    "  else:\n",
    "    object_name = o.capitalize()\n",
    "    # LOAD THE TRAINED MODEL FOR THE ACRUAL OBJECT\n",
    "    num = get_last_epoch(object_name)\n",
    "    print(\"LOADED MODEL: \" + str(num))\n",
    "    g = loadModel(\"gen\", num, object_name)\n",
    "    e = loadModel(\"enc\", num, object_name)\n",
    "    d = loadModel(\"dis\", num, object_name)\n",
    "\n",
    "  type_of_anomalies = []\n",
    "  # FOR EACH ANOMALIES IN THE OBJECT \n",
    "  for elem in dataset[o][\"test\"]:\n",
    "    type_of_anomalies.append(elem)\n",
    "\n",
    "  dim = 128\n",
    "  # FOR EACH IMAGE IN THE ANOMALY \n",
    "  for anomaly in type_of_anomalies:\n",
    "    print(\"\\t\"+anomaly.upper())\n",
    "\n",
    "    # TEMPORARY ARRARY TO STORE THE SCORE FOR EACH ANMALIES\n",
    "    # IN THE OBJECT O\n",
    "    per_type_score = []\n",
    "    per_type_y = []\n",
    "    \n",
    "    try:\n",
    "      if dim_img_resize == 128:\n",
    "        dataset = dataset_128\n",
    "      if dim_img_resize == 256:\n",
    "        dataset = dataset_256\n",
    "      if dim_img_resize == 512:\n",
    "        dataset = dataset_512\n",
    "      if dim_img_resize == 1024:\n",
    "        dataset = dataset_1024\n",
    "\n",
    "      if encoder:\n",
    "        model = anomaly_detector_encode(g=g, d=d, e=e, img_shape=(dim,dim,3))\n",
    "        intermidiate_model = feature_extractor_encode(d)  \n",
    "\n",
    "      index = 0\n",
    "      for elem in dataset[o][\"test\"][anomaly]:\n",
    "        if anomaly == \"good\":\n",
    "          y_true = 0\n",
    "        else:\n",
    "          y_true = 1\n",
    "\n",
    "        # DIVIDE THE IMAGE IN PATCHES OF DIMENSION dimXdim\n",
    "        blocks = np.array([elem[i:i+patches_dimension, j:j+patches_dimension] for j in range(0,dim_img_resize,patches_dimension) for i in range(0,dim_img_resize,patches_dimension)])\n",
    "        \n",
    "        # COMPUTE ANOMALY SCORE FOR EACH PATCHES OF THE IMAGES\n",
    "        patch_scores = []\n",
    "        max_score_patch = 0\n",
    "        for block in blocks:\n",
    "\n",
    "          # CREATE A RANDOM NORMAL NOISE \n",
    "          n1 = mynoise(1)\n",
    "          # USED TO COMPUTE THE TIME SPENT TO COMPUTE THE ANOMALY SCORE \n",
    "          start = cv2.getTickCount()\n",
    "\n",
    "          if encoder:\n",
    "            score, query, pred, diff= anomaly_detection_encode(block, n1, g=g, d=d, e=e, model=model, intermidiate_model=intermidiate_model)\n",
    "          else:\n",
    "            score, query, pred, diff = anomaly_detection(block, g=g, d=d, e=e, iteration=30)\n",
    "            #score, query, pred, diff = anomaly_detection_second(elem, g=g, d=d, iterations=30,)\n",
    "\n",
    "          patch_scores.append(score)\n",
    "          # FIND THE PATCH IN THE IMAGE THAT HAVE ANOMALY IN ORDER TO PLOT\n",
    "          # THIS PATCH, THE GENERATED IMAGE AND THE DIFFERENCE \n",
    "          if max_score_patch < score:\n",
    "            pred_to_plot = pred\n",
    "            query_to_plot = query\n",
    "            diff_to_plot = diff\n",
    "            max_score_patch = score\n",
    "\n",
    "          time = (cv2.getTickCount() - start) / cv2.getTickFrequency() * 100\n",
    "          \n",
    "        # PLOT THE QUERY, THE SIMILAR IMAGE THAT IS GENRATED FROM THE GENERATOR\n",
    "        # AND THE DIFFERENCE BETWEEN THE QUERY AND THE GENERATED IMAGE \n",
    "        f, axarr = plt.subplots(1,3)\n",
    "        # PLOT THE QUERY IMAGE\n",
    "        axarr[0].imshow(query_to_plot, cmap=\"viridis\")\n",
    "        axarr[0].axis(\"off\")\n",
    "        #axarr[0].title.set_text(\"Query\")\n",
    "        # PLOT THE GENERATED SIMILAR IMAGE\n",
    "        axarr[1].imshow(pred_to_plot, cmap=\"viridis\")\n",
    "        axarr[1].axis(\"off\")\n",
    "        #axarr[1].title.set_text(\"Generated\")\n",
    "        # PLOT THE DIFFERENCE \n",
    "        axarr[2].imshow(diff_to_plot, cmap=\"viridis\")\n",
    "        axarr[2].axis(\"off\") \n",
    "        #axarr[2].title.set_text(\"Difference\")\n",
    "        \n",
    "        # TITLE OF THE IMAGES\n",
    "        #title = anomaly.upper() + \" - ANOMALY SCORE: {} IN {} ms\".format(round(max_score_patch, 2), round(time, 2))\n",
    "        #plt.suptitle(title, fontsize=12, x = 0.5, y = 0.8)\n",
    "\n",
    "        # SAVE THE PLOT TO A PNG IMAGE \n",
    "        # ACCORDING IF WE HAVE USED THE ENCDER OR NO\n",
    "        if encoder:\n",
    "          plt.savefig(\"./result/wEncoder/\"+str(o)+\"/testing/\"+o+\"_\"+anomaly+\"_\"+str(index)+\".png\")\n",
    "        else:\n",
    "          plt.savefig(\"./result/woEncoder/\"+str(o)+\"/testing/\"+o+\"_\"+anomaly+\"_\"+str(index)+\".png\")\n",
    "\n",
    "        #plt.show()\n",
    "        #plt.close()\n",
    "        \n",
    "        score_list.append(np.max(np.array(patch_scores)))\n",
    "        Y_list.append(y_true)\n",
    "        per_type_score.append(np.max(np.array(patch_scores)))\n",
    "        per_type_y.append(y_true)\n",
    "\n",
    "        print(np.max(np.array(patch_scores)))\n",
    "\n",
    "        index += 1\n",
    "        \n",
    "    except Exception as exc:\n",
    "      print(exc)\n",
    "      print(\"ERROR: \" + anomaly)\n",
    "\n",
    "      if dim == 128:\n",
    "        dim = 256\n",
    "      else:\n",
    "        if dim == 256:\n",
    "          dim = 128\n",
    "\n",
    "        # IF THERE IS AN EXCEPTION, THE CURRENT TYPE OF ANOMALY WILL BE SKIPPED\n",
    "        # SO THE CURRENT ANOMALY IS ADDED TO THE END OF THE ARRAY\n",
    "        type_of_anomalies.append(anomaly)\n",
    "\n",
    "    partial_list_of_scores[o][anomaly] = per_type_score\n",
    "    partial_list_of_y[o][anomaly] = per_type_y\n",
    "\n",
    "  # SCORE FOR EACH TYPE OF ANOMALIES FOR EACH OBJECT\n",
    "  partial_list_of_scores[o][\"all\"] = score_list\n",
    "  partial_list_of_y[o][\"all\"] = Y_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiPOLfJwURYJ"
   },
   "source": [
    "# Testing anomaly detector without patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFvgDKDCJcMg"
   },
   "outputs": [],
   "source": [
    "# FLAG FOR USE THE ANOMALY DETECTOR WITH OT WITH OUT THE ENCODER\n",
    "encoder = True\n",
    "consistency = False\n",
    "dim = 128\n",
    "\n",
    "# LIST OF ALL OBJECT THAT WE MUST EVALUATE \n",
    "object_list = [\"carpet\"]\n",
    "\n",
    "# DICTONARY TO SAVE ALL THE SCORE OBTAINED FOR EACH OBJECT\n",
    "partial_list_of_scores = {}\n",
    "partial_list_of_y = {}\n",
    "\n",
    "# CREATE THE FOLDERS TO SAVE THE RESULT \n",
    "\n",
    "try:\n",
    "  os.mkdir(\"./result/\")\n",
    "  os.mkdir(\"./result/wEncoder\")\n",
    "  os.mkdir(\"./result/wEncoder/CBiGAN\")\n",
    "  os.mkdir(\"./result/wEncoder/BiGAN\")\n",
    "  os.mkdir(\"./result/woEncoder\")\n",
    "except:\n",
    "  pass\n",
    "\n",
    "for o in object_list:\n",
    "  # TEMPORARY ARRAY TO STORE THE SCORE COMPUTED FOR EACH OBJECT\n",
    "  score_list = []\n",
    "  Y_list = []\n",
    "\n",
    "  print(\"\\n\\nACTUAL OBJECT: \" + o)\n",
    "\n",
    "  try:\n",
    "    os.mkdir(\"./result/wEncoder/CBiGAN/\"+str(o))\n",
    "    os.mkdir(\"./result/wEncoder/BiGAN/\"+str(o))\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  try:\n",
    "    os.mkdir(\"./result/wEncoder/CBiGAN/\"+str(o)+\"/testing/\")\n",
    "    os.mkdir(\"./result/wEncoder/BiGAN/\"+str(o)+\"/testing/\")\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  partial_list_of_scores[o] = {}\n",
    "  partial_list_of_y[o] = {}\n",
    "\n",
    "  object_name = o.capitalize()\n",
    "  if consistency:\n",
    "    # LOAD THE TRAINED MODEL FOR THE ACRUAL OBJECT\n",
    "    num = get_last_epoch(object_name + \"_2\")\n",
    "    print(num)\n",
    "    g = loadModel(\"gen\", num, object_name + \"_2\")\n",
    "    e = loadModel(\"enc\", num, object_name + \"_2\")\n",
    "    d = loadModel(\"dis\", num, object_name + \"_2\")\n",
    "  else:\n",
    "    # LOAD THE TRAINED MODEL FOR THE ACRUAL OBJECT\n",
    "    num = get_last_epoch(object_name)\n",
    "    g = loadModel(\"gen\", num, object_name)\n",
    "    e = loadModel(\"enc\", num, object_name)\n",
    "    d = loadModel(\"dis\", num, object_name)\n",
    "\n",
    "  type_of_anomalies = []\n",
    "  # FOR EACH ANOMALIES IN THE OBJECT \n",
    "  for elem in dataset_128[o][\"test\"]:\n",
    "    type_of_anomalies.append(elem)\n",
    "\n",
    "  # FOR EACH IMAGE IN THE ANOMALY \n",
    "  for anomaly in type_of_anomalies:\n",
    "    print(\"\\t\"+anomaly.upper())\n",
    "\n",
    "    # TEMPORARY ARRARY TO STORE THE SCORE FOR EACH ANMALIES\n",
    "    # IN THE OBJECT O\n",
    "    per_type_score = []\n",
    "    per_type_y = []\n",
    "    \n",
    "    # SOME MODEL IS TRAINED WITH 256X256 DIMENSION IMAGES AND OTHERS WITH \n",
    "    # 128X128 IMAGES, SO THIS TRY-EXCEPT IS USED TO SWITCH THE DIMENSION IMAGES \n",
    "    try:\n",
    "      if dim == 128:\n",
    "        dataset = dataset_128\n",
    "      else:\n",
    "        dataset = dataset_256\n",
    "\n",
    "      if encoder:\n",
    "        model = anomaly_detector_encode(g=g, d=d, e=e, img_shape=(dim,dim,3))\n",
    "        intermidiate_model = feature_extractor_encode(d)  \n",
    "\n",
    "      index = 0\n",
    "      for elem in dataset[o][\"test\"][anomaly]:\n",
    "        if anomaly == \"good\":\n",
    "          y_true = 0\n",
    "        else:\n",
    "          y_true = 1\n",
    "        \n",
    "        # CREATE A RANDOM NORMAL NOISE \n",
    "        n1 = mynoise(1)\n",
    "        # USED TO COMPUTE THE TIME SPENT TO COMPUTE THE ANOMALY SCORE \n",
    "        start = cv2.getTickCount()\n",
    "\n",
    "        if encoder:\n",
    "          score, query, pred, diff= anomaly_detection_encode(elem, n1, g=g, d=d, e=e, model=model, intermidiate_model=intermidiate_model)\n",
    "        else:\n",
    "          score, query, pred, diff = anomaly_detection(elem, g=g, d=d, e=e, iteration=500)\n",
    "          #score, query, pred, diff = anomaly_detection_second(elem, g=g, d=d, iterations=30,)\n",
    "\n",
    "        time = (cv2.getTickCount() - start) / cv2.getTickFrequency() * 100\n",
    "        \n",
    "        # PLOT THE QUERY, THE SIMILAR IMAGE THAT IS GENRATED FROM THE GENERATOR\n",
    "        # AND THE DIFFERENCE BETWEEN THE QUERY AND THE GENERATED IMAGE \n",
    "        f, axarr = plt.subplots(1,3)\n",
    "\n",
    "        # PLOT THE QUERY IMAGE\n",
    "        axarr[0].imshow(query)\n",
    "        axarr[0].axis(\"off\")\n",
    "        axarr[0].title.set_text(\"Query\")\n",
    "        # PLOT THE GENERATED SIMILAR IMAGE\n",
    "        axarr[1].imshow(pred)\n",
    "        axarr[1].axis(\"off\")\n",
    "        axarr[1].title.set_text(\"Generated\")\n",
    "        # PLOT THE DIFFERENCE \n",
    "        axarr[2].imshow(diff, cmap=\"viridis\")\n",
    "        #axarr[2].plot(0, 0, label=\"score\"+str(round(score, 2)))\n",
    "        axarr[2].axis(\"off\") \n",
    "        axarr[2].title.set_text(\"Difference\")\n",
    "        #axarr[2].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0. )\n",
    "\n",
    "        title = anomaly.upper() + \" - ANOMALY SCORE: {} IN {} ms\".format(round(score, 2), round(time, 2))\n",
    "        \n",
    "        # TITLE OF THE IMAGES\n",
    "        #plt.suptitle(title, fontsize=12, x = 0.5, y = 0.1)\n",
    "\n",
    "        # SAVE THE PLOT TO A PNG IMAGE \n",
    "        # ACCORDING IF WE HAVE USED THE ENCDER OR NO\n",
    "        f.set_size_inches(7, 3)\n",
    "\n",
    "        if consistency:\n",
    "           plt.savefig(\"./result/wEncoder/CBiGAN/\"+str(o)+\"/testing/\"+o+\"_\"+anomaly+\"_\"+str(index)+\".png\")\n",
    "        else:\n",
    "          plt.savefig(\"./result/wEncoder/BiGAN/\"+str(o)+\"/testing/\"+o+\"_\"+anomaly+\"_\"+str(index)+\".png\")\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "        score_list.append(score)\n",
    "        Y_list.append(y_true)\n",
    "        per_type_score.append(score)\n",
    "        per_type_y.append(y_true)\n",
    "\n",
    "        index += 1\n",
    "    except Exception as exc:\n",
    "      print(exc)\n",
    "      print(\"ERROR: \" + anomaly)\n",
    "\n",
    "      if dim == 128:\n",
    "        dim = 256\n",
    "      else:\n",
    "        if dim == 256:\n",
    "          dim = 128\n",
    "\n",
    "      # IF THERE IS AN EXCEPTION, THE CURRENT TYPE OF ANOMALY WILL BE SKIPPED\n",
    "      # SO THE CURRENT ANOMALY IS ADDED TO THE END OF THE ARRAY\n",
    "      type_of_anomalies.append(anomaly)\n",
    "\n",
    "    partial_list_of_scores[o][anomaly] = per_type_score\n",
    "    partial_list_of_y[o][anomaly] = per_type_y\n",
    "\n",
    "  # SCORE FOR EACH TYPE OF ANOMALIES FOR EACH OBJECT\n",
    "  partial_list_of_scores[o][\"all\"] = score_list\n",
    "  partial_list_of_y[o][\"all\"] = Y_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0r6YAM6hOasX"
   },
   "source": [
    "# Mean Classification Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eknJUzawBv6O"
   },
   "outputs": [],
   "source": [
    "for o in partial_list_of_scores:\n",
    "  print(\"CURRENT OBJECT IS: \" + str(o).upper())\n",
    "  acc_list_anomaly = []\n",
    "  acc_list_good = []\n",
    "  th_list = []\n",
    "  th = 0\n",
    "  while(th < np.mean(partial_list_of_scores[o][\"good\"]) * 2):\n",
    "    list_of_acc = []\n",
    "\n",
    "    # COMPUTE ACCURACY FOR ANOMALY CATEGORY\n",
    "    for anomaly in partial_list_of_scores[o]:\n",
    "      if anomaly != \"all\" and anomaly != \"good\":\n",
    "        th = th + 1\n",
    "        #norm_score_list = (partial_list_of_scores[o][anomaly]-np.min(partial_list_of_scores[o][anomaly]))/(np.max(partial_list_of_scores[o][anomaly])-np.min(partial_list_of_scores[o][anomaly]))\n",
    "        norm_score_list = partial_list_of_scores[o][anomaly]\n",
    "        y_predict = []\n",
    "\n",
    "        for elem in norm_score_list:\n",
    "          if elem > th:\n",
    "            y_predict.append(1)\n",
    "          else:\n",
    "            y_predict.append(0)\n",
    "\n",
    "        i = 0\n",
    "        corrected_classified = 0\n",
    "        for elem in y_predict:\n",
    "          if elem == partial_list_of_y[o][anomaly][i]:\n",
    "            corrected_classified += 1 \n",
    "          i+=1\n",
    "\n",
    "        corrected_classified = corrected_classified/len(y_predict)\n",
    "        list_of_acc.append(corrected_classified)\n",
    "  \n",
    "    # COMPUTE THE ACCURACY FOR THE GOOD CATEGORY\n",
    "    #norm_score_list = (partial_list_of_scores[o][\"good\"]-np.min(partial_list_of_scores[o][\"good\"]))/(np.max(partial_list_of_scores[o][\"good\"])-np.min(partial_list_of_scores[o][\"good\"]))\n",
    "    norm_score_list = partial_list_of_scores[o][\"good\"]\n",
    "    y_predict = []\n",
    "    for elem in norm_score_list:\n",
    "      if elem > th:\n",
    "        y_predict.append(1)\n",
    "      else:\n",
    "        y_predict.append(0)\n",
    "\n",
    "    i = 0\n",
    "    corrected_classified = 0\n",
    "    for elem in y_predict:\n",
    "      if elem == partial_list_of_y[o][\"good\"][i]:\n",
    "        corrected_classified += 1 \n",
    "      i+=1\n",
    "    corrected_classified = corrected_classified/len(y_predict)\n",
    "\n",
    "    # GOOD ACCURACY\n",
    "    mean_acc_good = corrected_classified\n",
    "    # ANOMALY ACCURACY\n",
    "    mean_acc_anomaly = np.mean(list_of_acc)\n",
    "\n",
    "    # COMPUTE THE OVERALL ACCURACY \n",
    "    acc = (mean_acc_good + mean_acc_anomaly)/2\n",
    "\n",
    "    acc_list_anomaly.append(mean_acc_anomaly)\n",
    "    acc_list_good.append(mean_acc_good)\n",
    "    th_list.append(th)\n",
    "    #print(\"ACCURACY :\" + str(round(acc, 3)))\n",
    "\n",
    "acc_intersection = 0\n",
    "i = 0\n",
    "# FIND THE EQUAL ERROR RATE \n",
    "for elem in acc_list_anomaly:\n",
    "  #print(str(round(elem, 2)) + \" - \" + str(round(acc_list_good[i], 2)))\n",
    "  if round(elem, 1) == round(acc_list_good[i], 1):\n",
    "    if acc_intersection < elem:\n",
    "      acc_intersection = elem\n",
    "      th_intersection = th_list[i]\n",
    "  i+=1\n",
    "\n",
    "# FIND THE MAX ACCURACY \n",
    "i = 0\n",
    "max_acc = 0\n",
    "max_threashold = 0\n",
    "max_index = 0\n",
    "max_acc_list = []\n",
    "for elem in acc_list_anomaly:\n",
    "  mean = (elem + acc_list_good[i])/2\n",
    "  max_acc_list.append(mean)\n",
    "  if max_acc < mean:\n",
    "    max_acc = mean\n",
    "    max_threashold = th_list[i]\n",
    "    max_index = i\n",
    "  i+=1\n",
    "\n",
    "print(\"MAX ACCURACY ACHIEVED: \" + str(round(max_acc, 3)))\n",
    "print(\"THRESHOLD:\" + str(max_threashold))\n",
    "print(\"GOOD ACCURACY:\" + str(acc_list_good[max_index]))\n",
    "print(\"ANOMALY ACCURACY:\" + str(acc_list_anomaly[max_index]))\n",
    "\n",
    "# GET THE INDEX OF THE ELEMENT THAT IS THE MAX IN THE ARRAY\n",
    "# IN ORDER TO GET THE THRESHOLD \n",
    "#index  = acc_list_good.index(acc_intersection)\n",
    "#th_intersection = th_list[index]\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "plt.plot(th_list, max_acc_list, color=\"purple\", label = \"mean accuracy\")\n",
    "plt.axvline(x=max_threashold, color=\"grey\", linestyle=\"--\", label = \"max accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# PLOT THE VARIATION OF THE ACCURACY WHEN WE VARY THE TH \n",
    "plt.plot(th_list, acc_list_anomaly, color=\"purple\", label=\"anomaly accuracy\")\n",
    "plt.plot(th_list, acc_list_good, color=\"green\", label=\"good accuracy\")\n",
    "#plt.plot(th_intersection, acc_intersection, markersize=10, marker=\"x\", color=\"red\", label=\"intersection\")\n",
    "plt.axvline(x=max_threashold, color=\"grey\", linestyle=\"--\", label = \"max accuracy = \" + str(round(max_acc, 3)))\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(1)\n",
    "#plt.axhline(y=acc_intersection, color=\"orange\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "#plt.title(o.upper() + \": accuracy achieved: \" + str(round(max_acc, 3)) + \" - with threshold equalt to: \" + str(round(max_threashold, 3)), y = 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vG1_LG3E9-Vc"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "x = stats.zscore(score_list)\n",
    "print(x)\n",
    "dim = len(dataset_128[\"toothbrush\"][\"test\"][\"good\"])\n",
    "sns.distplot(x)\n",
    "plt.show()\n",
    "plt.close()\n",
    "sns.distplot(norm_score_list)\n",
    "m = np.mean(score_list[:dim])\n",
    "plt.plot(score_list[:dim])\n",
    "plt.axhline(y=m)\n",
    "\n",
    "y_predict = []\n",
    "\n",
    "for elem in score_list:\n",
    "  if elem > dim:\n",
    "    y_predict.append(1)\n",
    "  else:\n",
    "    y_predict.append(0)\n",
    "\n",
    "error = 0\n",
    "i = 0\n",
    "for elem in y_predict:\n",
    "  \n",
    "  if elem != Y_list[i]:\n",
    "    error += 1 \n",
    "\n",
    "  i+=1\n",
    "\n",
    "error = error/len(y_predict)\n",
    "print(\"fdfsdfsd\")\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjc4pZ78C644"
   },
   "source": [
    "# ROC curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "jM5lpj544YVQ",
    "outputId": "d8263405-002e-424a-edfa-9a33e9e4295e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(color_codes=True)\n",
    "\n",
    "for object_name in partial_list_of_scores:\n",
    "  try:\n",
    "    os.mkdir(\"./result/wEncoder/\"+str(object_name))\n",
    "    os.mkdir(\"./result/woEncoder/\"+str(object_name))\n",
    "  except:\n",
    "    pass\n",
    "    \n",
    "  for anomaly in partial_list_of_scores[object_name]:\n",
    "    if anomaly != \"good\" and anomaly != \"all\":\n",
    "      score = np.concatenate((partial_list_of_scores[object_name][\"good\"], partial_list_of_scores[object_name][anomaly]))\n",
    "      label = np.concatenate((partial_list_of_y[object_name][\"good\"], partial_list_of_y[object_name][anomaly]))\n",
    "\n",
    "      fpr, tpr, threshold = roc_curve(label, score)\n",
    "\n",
    "      roc_auc = auc(fpr, tpr)\n",
    "      plt.figure()\n",
    "      lw = 2\n",
    "      plt.plot(fpr, tpr, color='red', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "      plt.plot([0, 1], [0, 1], color='m', lw=lw, linestyle='--')\n",
    "      plt.xlim([0.0, 1.0])\n",
    "      plt.ylim([0.0, 1.05])\n",
    "      plt.xlabel('False Positive Rate')\n",
    "      plt.ylabel('True Positive Rate')\n",
    "      plt.legend(loc=\"lower right\")\n",
    "\n",
    "      if encoder:\n",
    "        plt.savefig('./result/wEncoder/'+object_name+'/ROC_'+object_name+'_'+anomaly+'.png')\n",
    "      else:\n",
    "        plt.savefig('./result/woEncoder/'+object_name+'/ROC_'+object_name+'_'+anomaly+'.png')\n",
    "\n",
    "      plt.close()\n",
    "\n",
    "      ax = sns.kdeplot(partial_list_of_scores[object_name][\"good\"], shade=True, color=\"m\", label=\"normal\")\n",
    "      #ax = sns.kdeplot(partial_list_of_scores[object_name][anomaly], shade=True, color=\"r\", label=\"anomaly\").set_title(anomaly)\n",
    "      ax = sns.kdeplot(partial_list_of_scores[object_name][anomaly], color=\"r\",shade=True, label=anomaly)\n",
    "\n",
    "      fig = ax.get_figure()\n",
    "\n",
    "      if encoder:\n",
    "        fig.savefig('./result/wEncoder/'+object_name+'/DISTR_'+object_name+'_'+anomaly+'.png')\n",
    "      else:\n",
    "        fig.savefig('./result/woEncoder/'+object_name+'/DISTR_'+object_name+'_'+anomaly+'.png')\n",
    "      fig.clear()\n",
    "\n",
    "    fpr, tpr, threshold = roc_curve(partial_list_of_y[object_name][\"all\"], partial_list_of_scores[object_name][\"all\"])\n",
    "\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='red', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='m', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    if encoder:\n",
    "      plt.savefig('./result/wEncoder/'+object_name+'/ROC_'+object_name+'_ALL.png')\n",
    "    else:\n",
    "      plt.savefig('./result/woEncoder/'+object_name+'/ROC_'+object_name+'_ALL.png')\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AVVh_c9WDLN"
   },
   "source": [
    "# Visualization of BiGAN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVQzfXkEWB1k"
   },
   "outputs": [],
   "source": [
    "object_name = \"Metal_nut\"\n",
    "\n",
    "# LOAD THE TRAINED MODEL FOR THE ACRUAL OBJECT\n",
    "num = get_last_epoch(object_name + \"_2\")\n",
    "print(\"LOADED MODEL: \" + str(num))\n",
    "g = loadModel(\"gen\", num, object_name + \"_2\")\n",
    "e = loadModel(\"enc\", num, object_name + \"_2\")\n",
    "d = loadModel(\"dis\", num, object_name + \"_2\")\n",
    "\n",
    "# GENERATE IMAGE FROM RANDOM NOISE \n",
    "batch_images = []\n",
    "for i in range(0,5):\n",
    "  z = np.random.uniform(0, 1, size=(1, 128))\n",
    "  img = g.predict(z)\n",
    "  batch_images.append(img[0])\n",
    "\n",
    "f, axarr = plt.subplots(1,5)\n",
    "for i in range(0,5):\n",
    "  axarr[i].imshow(batch_images[i])\n",
    "  axarr[i].axis(\"off\")\n",
    "\n",
    "title = \"Cable image generated from random noise\"\n",
    "\n",
    "# TITLE OF THE IMAGES\n",
    "plt.suptitle(title, fontsize=12, x = 0.5, y = 1)\n",
    "f.set_size_inches(10, 2)\n",
    "plt.savefig(\"./generated_image_without_encoder4.png\")\n",
    "\n",
    "# GENERATE IMAGE FROM ENCODER EMBEDDING \n",
    "batch_images = []\n",
    "for i in range(0 ,5):\n",
    "  query_img = dataset_128[object_name.lower()][\"test\"][\"good\"][i]\n",
    "  quety_img_shape = query_img.shape\n",
    "  query_img = query_img.reshape(1, quety_img_shape[0], quety_img_shape[1], quety_img_shape[2])\n",
    "\n",
    "  generated_noise = e.predict(query_img)\n",
    "  img = g.predict(generated_noise)\n",
    "  batch_images.append(img[0])\n",
    "\n",
    "f, axarr = plt.subplots(1,5)\n",
    "for i in range(0,5):\n",
    "  axarr[i].imshow(batch_images[i])\n",
    "  axarr[i].axis(\"off\")\n",
    "\n",
    "# TITLE OF THE IMAGES\n",
    "plt.suptitle(title, fontsize=12, x = 0.5, y = 1)\n",
    "f.set_size_inches(10, 2)\n",
    "plt.savefig(\"./generated_image_with_encoder4.png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tcn-YU088ZQY"
   },
   "outputs": [],
   "source": [
    "from skimage.measure import compare_ssim\n",
    "import argparse\n",
    "import imutils\n",
    "import cv2\n",
    "\n",
    "# 3. Load the two input images\n",
    "for elem in X_test:\n",
    "  imageA = elem\n",
    "  temp = []\n",
    "  temp.append(imageA)\n",
    "  gen_noise = e.predict(np.array(temp))\n",
    "  gen_img = g.predict(gen_noise)\n",
    "  imageB = gen_img[0]\n",
    "\n",
    "  # 4. Convert the images to grayscale\n",
    "  grayA = cv2.cvtColor(imageA, cv2.COLOR_BGR2GRAY)\n",
    "  grayB = cv2.cvtColor(imageB, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "  # 5. Compute the Structural Similarity Index (SSIM) between the two\n",
    "  #    images, ensuring that the difference image is returned\n",
    "  (score, diff) = compare_ssim(grayA, grayB, full=True)\n",
    "  diff = (diff * 255).astype(\"uint8\")\n",
    "\n",
    "  plt.imshow(diff)\n",
    "  plt.show()\n",
    "\n",
    "  # 6. You can print only the score if you want\n",
    "  print(\"SSIM: {}\".format(score))\n",
    "\n",
    "  score_list.append(score)\n",
    "  Y_list.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PubkOT-_-gpb"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from scipy import signal\n",
    "from scipy import ndimage\n",
    "\n",
    "def gaussian2(size, sigma):\n",
    "  \"\"\"Returns a normalized circularly symmetric 2D gauss kernel array\n",
    "\n",
    "  f(x,y) = A.e^{-(x^2/2*sigma^2 + y^2/2*sigma^2)} where\n",
    "\n",
    "  A = 1/(2*pi*sigma^2)\n",
    "\n",
    "  as define by Wolfram Mathworld \n",
    "  http://mathworld.wolfram.com/GaussianFunction.html\n",
    "  \"\"\"\n",
    "  A = 1/(2.0*numpy.pi*sigma**2)\n",
    "  x, y = numpy.mgrid[-size//2 + 1:size//2 + 1, -size//2 + 1:size//2 + 1]\n",
    "  g = A*numpy.exp(-((x**2/(2.0*sigma**2))+(y**2/(2.0*sigma**2))))\n",
    "  return g\n",
    "\n",
    "def fspecial_gauss(size, sigma):\n",
    "  \"\"\"Function to mimic the 'fspecial' gaussian MATLAB function\n",
    "  \"\"\"\n",
    "  x, y = numpy.mgrid[-size//2 + 1:size//2 + 1, -size//2 + 1:size//2 + 1]\n",
    "  g = numpy.exp(-((x**2 + y**2)/(2.0*sigma**2)))\n",
    "  return g/g.sum()\n",
    "\n",
    "def msssim(img1, img2):\n",
    "  \"\"\"This function implements Multi-Scale Structural Similarity (MSSSIM) Image \n",
    "  Quality Assessment according to Z. Wang's \"Multi-scale structural similarity \n",
    "  for image quality assessment\" Invited Paper, IEEE Asilomar Conference on \n",
    "  Signals, Systems and Computers, Nov. 2003 \n",
    "  \n",
    "  Author's MATLAB implementation:-\n",
    "  http://www.cns.nyu.edu/~lcv/ssim/msssim.zip\n",
    "  \"\"\"\n",
    "  level = 5\n",
    "  weight = numpy.array([0.0448, 0.2856, 0.3001, 0.2363, 0.1333])\n",
    "  downsample_filter = numpy.ones((2, 2))/4.0\n",
    "  im1 = img1.astype(numpy.float64)\n",
    "  im2 = img2.astype(numpy.float64)\n",
    "  mssim = numpy.array([])\n",
    "  mcs = numpy.array([])\n",
    "  for l in range(level):\n",
    "      ssim_map, cs_map = ssim(im1, im2, cs_map=True)\n",
    "      mssim = numpy.append(mssim, ssim_map.mean())\n",
    "      mcs = numpy.append(mcs, cs_map.mean())\n",
    "      filtered_im1 = ndimage.filters.convolve(im1, downsample_filter, \n",
    "                                              mode='reflect')\n",
    "      filtered_im2 = ndimage.filters.convolve(im2, downsample_filter, \n",
    "                                              mode='reflect')\n",
    "      im1 = filtered_im1[::2, ::2]\n",
    "      im2 = filtered_im2[::2, ::2]\n",
    "  return (numpy.prod(mcs[0:level-1]**weight[0:level-1])*\n",
    "                  (mssim[level-1]**weight[level-1]))\n",
    "\n",
    "def ssim(img1, img2, cs_map=False):\n",
    "  print(img1.shape)\n",
    "  print(img2.shape)\n",
    "  \"\"\"Return the Structural Similarity Map corresponding to input images img1 \n",
    "  and img2 (images are assumed to be uint8)\n",
    "\n",
    "  This function attempts to mimic precisely the functionality of ssim.m a \n",
    "  MATLAB provided by the author's of SSIM\n",
    "  https://ece.uwaterloo.ca/~z70wang/research/ssim/ssim_index.m\n",
    "  \"\"\"\n",
    "  img1 = img1.astype(numpy.float64)\n",
    "  img2 = img2.astype(numpy.float64)\n",
    "  size = 11\n",
    "  sigma = 1.5\n",
    "  window = fspecial_gauss(size, sigma)\n",
    "  K1 = 0.01\n",
    "  K2 = 0.03\n",
    "  L = 255 #bitdepth of image\n",
    "  C1 = (K1*L)**2\n",
    "  C2 = (K2*L)**2\n",
    "  print(window.shape)\n",
    "  mu1 = signal.fftconvolve(window, img1, mode='valid')\n",
    "  mu2 = signal.fftconvolve(window, img2, mode='valid')\n",
    "  mu1_sq = mu1*mu1\n",
    "  mu2_sq = mu2*mu2\n",
    "  mu1_mu2 = mu1*mu2\n",
    "  sigma1_sq = signal.fftconvolve(window, img1*img1, mode='valid') - mu1_sq\n",
    "  sigma2_sq = signal.fftconvolve(window, img2*img2, mode='valid') - mu2_sq\n",
    "  sigma12 = signal.fftconvolve(window, img1*img2, mode='valid') - mu1_mu2\n",
    "  if cs_map:\n",
    "      return (((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*\n",
    "                  (sigma1_sq + sigma2_sq + C2)), \n",
    "              (2.0*sigma12 + C2)/(sigma1_sq + sigma2_sq + C2))\n",
    "  else:\n",
    "      return ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*\n",
    "                  (sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "imageA = X_test[6]\n",
    "temp = []\n",
    "temp.append(imageA)\n",
    "gen_noise = e.predict(np.array(temp))\n",
    "gen_img = g.predict(gen_noise)\n",
    "imageB = gen_img[0]\n",
    "\n",
    "imageA = cv2.cvtColor(imageA, cv2.COLOR_BGR2GRAY)\n",
    "imageB = cv2.cvtColor(imageB, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "image1 = numpy.array(imageA)\n",
    "image2 = numpy.array(imageB)\n",
    "\n",
    "ssim_map = ssim(image1, image2)\n",
    "ms_ssim = msssim(image1, image2)\n",
    "\n",
    "import pylab\n",
    "\n",
    "pylab.figure()\n",
    "pylab.subplot(131)\n",
    "pylab.title('Image1')\n",
    "pylab.imshow(imageA, interpolation='nearest', cmap=pylab.gray())\n",
    "pylab.subplot(132)\n",
    "pylab.title('Image2')\n",
    "pylab.imshow(imageB, interpolation='nearest', cmap=pylab.gray())\n",
    "pylab.subplot(133)\n",
    "pylab.title('SSIM Map\\n SSIM: %f\\n MSSSIM: %f' % (ssim_map.mean(), ms_ssim))\n",
    "pylab.imshow(ssim_map, interpolation='nearest', cmap=pylab.gray())\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "91-plvVW_c4L"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from skimage import io\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def log10(x):\n",
    "    numerator = tf.log(x)\n",
    "    denominator = tf.log(tf.constant(10, dtype=numerator.dtype))\n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "def psnr(im1, im2):\n",
    "    img_arr1 = numpy.array(im1).astype('float32')\n",
    "    img_arr2 = numpy.array(im2).astype('float32')\n",
    "    mse = tf.reduce_mean(tf.squared_difference(img_arr1, img_arr2))\n",
    "    psnr = tf.constant(255**2, dtype=tf.float32)/mse\n",
    "    result = tf.constant(10, dtype=tf.float32)*log10(psnr)\n",
    "    with tf.Session():\n",
    "        result = result.eval()\n",
    "    return result\n",
    "\n",
    "imageA = X_test[4]\n",
    "temp = []\n",
    "temp.append(imageA)\n",
    "gen_noise = e.predict(np.array(temp))\n",
    "gen_img = g.predict(gen_noise)\n",
    "imageB = gen_img[0]\n",
    "\n",
    "psnr(imageA, imageB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3edkW3lE1DH"
   },
   "outputs": [],
   "source": [
    "python -v"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "5Wt8qIXvo2L1",
    "Iye-usQdoxQ6",
    "_EfSgU10nhv5",
    "RxMuS6XBbCRr",
    "uF59TH7mbJDy",
    "jCzn68WjPCbH",
    "hDIrdkbMUam6",
    "0r6YAM6hOasX",
    "mjc4pZ78C644",
    "6AVVh_c9WDLN",
    "DjQ95L1gtq-m"
   ],
   "machine_shape": "hm",
   "name": "ConsistencyBiGAN_3.7.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
